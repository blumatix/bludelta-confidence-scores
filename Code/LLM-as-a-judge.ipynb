{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22c8dcc",
   "metadata": {},
   "source": [
    "# Implementation Roadmap\n",
    "## LLM as a judge\n",
    "### Must-have (ship first)\n",
    "- **Strict JSON schema + retry-on-fail**  \n",
    "  Ensures well-formed, consistent outputs.  \n",
    "\n",
    "- **Multi-trace CoT (k=3–5) + self-confidence**  \n",
    "  Generate multiple reasoning traces; each trace provides its own confidence score.  \n",
    "\n",
    "- **Weighted voting by self-confidence**  \n",
    "  Aggregate values using confidence weights instead of plain majority.  \n",
    "\n",
    "- **Evidence grounding (span linking)**  \n",
    "  Judge must cite exact supporting text spans; missing evidence → lower confidence.  \n",
    "\n",
    "- **Domain validators (hard/soft rules)**  \n",
    "  - Dates follow expected formats  \n",
    "  - VAT ID / IBAN checksum validation  \n",
    "  - `Gross ≈ Net + VAT` (within tolerance)  \n",
    "  - Currency whitelist checks  \n",
    "\n",
    "- **Per-field calibration**  \n",
    "  Map raw confidence to true correctness probability (e.g., isotonic or temperature scaling).  \n",
    "\n",
    "- **Acceptance policy**  \n",
    "  Apply per-field thresholds; for documents, require all critical fields to pass.  \n",
    "\n",
    "- **Auditing & provenance**  \n",
    "  Store chosen value, calibrated confidence, cited spans, validator outcomes, and reasoning traces.  \n",
    "\n",
    "---\n",
    "\n",
    "### High-impact add-ons (still no training)\n",
    "- **Dual-phase reasoning**  \n",
    "  1. Solution reasoning → generate answer  \n",
    "  2. Confidence reasoning → judge and verbalize confidence bin (0–9 or 0–1)  \n",
    "\n",
    "- **Difficulty-aware sampling**  \n",
    "  Use vote dispersion or evidence quality to stop early on easy fields or sample more traces for hard ones.  \n",
    "\n",
    "- **Cross-judge diversity (multiple rubrics)**  \n",
    "  Run the same model with different judging rubrics and combine results:  \n",
    "  - *Format-strict judge*: strict regex/format checks (e.g., dates, VAT IDs)  \n",
    "  - *Arithmetic-strict judge*: check totals and sums (e.g., Gross vs. Net + VAT)  \n",
    "  - *Evidence-strict judge*: require supporting spans near relevant anchors (e.g., “VAT”, “Total”)  \n",
    "\n",
    "- **Few-shot bank (prompt-only)**  \n",
    "  Maintain a curated set of vendor/layout examples; update as new reviewed cases are added.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ops & monitoring\n",
    "- **Coverage vs. accuracy dashboards**  \n",
    "  Track Brier, ECE, NLL, AUROC; monitor auto-accept vs. review rates.  \n",
    "\n",
    "- **Periodic recalibration**  \n",
    "  Refresh calibration mappings as new reviewed cases arrive.  \n",
    "\n",
    "- **Fail-safes**  \n",
    "  If JSON invalid or confidence unstable → deterministic re-ask at temperature=0; else route to review.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052830f",
   "metadata": {},
   "source": [
    "<style>\n",
    "open { color: Red }\n",
    "inprogress { color: Yellow }\n",
    "done { color: Green; text-decoration: line-through }\n",
    "</style>\n",
    "\n",
    "# First steps\n",
    "To start things off, we need to build a simple LLM-as-a-Judge (LaaJ) setup<br>\n",
    "For this we need to do prepare the following:\n",
    "- <done>Access to the Azure hosted GPT5-mini</done>\n",
    "- <inprogress>Access to the data located on the NAS</inprogress>\n",
    "\n",
    "Then we need to force the LLM to create multiple reasoning paths for one input.<br>\n",
    "- <done>Actively encourage the LLM to perform \"slow thinking\" and prepare a json output schema the data needs to fit in.</done><br>\n",
    "- <done>Implement a retry option (with amount of retries as a parameter) to catch LLM errors.</done><br>\n",
    "- <open>Peform validity checks for produced outputs (Valid VAT layout, isNumber for amounts, ...). If these checks fail --> confidence decrease / set to 0.</open><br>\n",
    "- <open>Query the LLM to rate its own output using numeric and written bins (10 bins, [0.0; 0.1], [0.1; 0.2], ...)</open><br>\n",
    "- <open>Implement weighted majority voting based on confidence (Borda voting with exponent)</open><br>\n",
    "\n",
    "<open>Visualize the results of perceived confidence and actual correctness in a bar chart (should be x=y for perfect prediction)</open><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653a805",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0648315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from openai import AsyncOpenAI, RateLimitError\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv(override = True)\n",
    "# ---------------------------------HELPER-VARS--------------------------------\n",
    "DOCUMENT_STORAGE = Path(os.getenv(\"DOCUMENT_STORAGE\"))\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# ---------------------------------LLM-CONFIG---------------------------------\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# ---------------------------------LLM-SETUP---------------------------------\n",
    "client = AsyncOpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=ENDPOINT\n",
    ")\n",
    "\n",
    "CONCCURRENT_TASKS = 15\n",
    "REQUEST_DELAY = 1 # seconds\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BACKOFF = 5  # seconds\n",
    "\n",
    "# Whitelist of retryable exceptions\n",
    "RETRYABLE_EXCEPTIONS = (\n",
    "    RateLimitError,\n",
    "    httpx.ConnectTimeout,\n",
    "    httpx.ReadTimeout,\n",
    "    httpx.HTTPStatusError,\n",
    "    httpx.RemoteProtocolError,\n",
    "    httpx.NetworkError,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------LOGGING---------------------------------\n",
    "logging_path = Path(\"../Logs\")\n",
    "if not logging_path.exists():\n",
    "    logging_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=f\"{logging_path}/{DEPLOYMENT_NAME}_{TIMESTAMP}.log\",\n",
    "    filemode=\"a\",                     \n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    force=True\n",
    ")\n",
    "\n",
    "for noisy in [\"httpx\", \"openai\", \"azure\", \"urllib3\"]:\n",
    "    logging.getLogger(noisy).setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Load system prompt\n",
    "with open(\"../Prompts/system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "# Load the schema from file\n",
    "with open(\"../Prompts/schema.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "# Turn it into pretty JSON for prompt injection\n",
    "formatted_schema = json.dumps(schema, indent=2)\n",
    "\n",
    "system_prompt = system_prompt.replace(\"formatted_schema_representation\",formatted_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acf4a9",
   "metadata": {},
   "source": [
    "# Test Reasoning path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0a8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPT-5 via Responses API: system prompt + hardcoded candidate file ---\n",
    "\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# 1) Hardcode the candidate file path\n",
    "CANDIDATE_TEXT_PATH = Path(\n",
    "    \"../Documents/273366/273277_page1_grid.txt\"\n",
    ")  # <-- adjust as needed\n",
    "\n",
    "\n",
    "def _read_candidate_text(p: Path) -> str:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Candidate file not found: {p}\")\n",
    "    return p.read_text(encoding=\"utf-8\").strip()\n",
    "\n",
    "\n",
    "async def _responses_create_with_retry(\n",
    "    instructions: str,\n",
    "    user_text: str,\n",
    "    *,\n",
    "    reasoning_effort: str = \"minimal\",  # GPT-5 supports 'minimal' | 'low' | 'medium' | 'high'\n",
    "    verbosity: str = \"low\",  # GPT-5 supports 'low' | 'medium' | 'high'\n",
    "    max_output_tokens: int = 4000,\n",
    ") -> Tuple[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Calls the Azure Responses API (GPT-5) with retries.\n",
    "    Returns (assistant_text, reasoning_summary_text_or_none).\n",
    "    \"\"\"\n",
    "    last_err: Optional[Exception] = None\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(DEPLOYMENT_NAME)\n",
    "            resp = await client.responses.create(\n",
    "                model=DEPLOYMENT_NAME,  # <- your GPT-5 (or gpt-5-mini) *deployment name*\n",
    "                instructions=instructions,  # <- system prompt goes here in Responses API\n",
    "                input=user_text,  # <- user content (the candidate text)\n",
    "                max_output_tokens=max_output_tokens,\n",
    "                # Reasoning-specific controls for GPT-5:\n",
    "                reasoning={\n",
    "                    \"effort\": reasoning_effort,\n",
    "                    \"summary\": \"auto\",  # request a reasoning summary when available\n",
    "                },\n",
    "                # GPT-5 output verbosity control:\n",
    "                text={\"verbosity\": verbosity},\n",
    "            )\n",
    "\n",
    "            # Primary text\n",
    "            out_text = getattr(resp, \"output_text\", None)\n",
    "            if not out_text:\n",
    "                # Fallback: concatenate all output_text parts if needed\n",
    "                parts = []\n",
    "                for item in resp.output or []:\n",
    "                    if getattr(item, \"type\", \"\") == \"message\":\n",
    "                        for c in getattr(item, \"content\", []) or []:\n",
    "                            if getattr(c, \"type\", \"\") in (\n",
    "                                \"output_text\",\n",
    "                                \"text\",\n",
    "                            ) and getattr(c, \"text\", None):\n",
    "                                parts.append(c.text)\n",
    "                out_text = \"\\n\".join(parts).strip() if parts else \"\"\n",
    "\n",
    "            # Optional: extract a short reasoning summary if present\n",
    "            reasoning_summary = None\n",
    "            for item in resp.output or []:\n",
    "                if getattr(item, \"type\", \"\") == \"reasoning\":\n",
    "                    # item.summary may be a list of structured segments; join any text fields\n",
    "                    segs = []\n",
    "                    for seg in getattr(item, \"summary\", []) or []:\n",
    "                        t = getattr(seg, \"text\", None)\n",
    "                        if t:\n",
    "                            segs.append(t)\n",
    "                    if segs:\n",
    "                        reasoning_summary = \"\\n\".join(segs).strip()\n",
    "                        break\n",
    "\n",
    "            logging.info(\n",
    "                \"Responses API success on attempt %d | tokens: %s\",\n",
    "                attempt,\n",
    "                getattr(resp, \"usage\", None),\n",
    "            )\n",
    "            return out_text, reasoning_summary\n",
    "\n",
    "        except RETRYABLE_EXCEPTIONS as e:\n",
    "            last_err = e\n",
    "            logging.warning(\n",
    "                \"Responses API retry %d/%d due to %s\", attempt, MAX_RETRIES, repr(e)\n",
    "            )\n",
    "            await asyncio.sleep(RETRY_BACKOFF * attempt)\n",
    "\n",
    "    # If we exhausted retries, raise the last error\n",
    "    raise RuntimeError(\n",
    "        f\"Responses API failed after {MAX_RETRIES} attempts\"\n",
    "    ) from last_err\n",
    "\n",
    "\n",
    "# 2) One-shot helper you can call from a notebook cell\n",
    "async def run_gpt5_on_candidate(\n",
    "    reasoning_effort: str = \"minimal\",  # GPT-5 supports 'minimal' | 'low' | 'medium' | 'high'\n",
    "    verbosity: str = \"low\",\n",
    ") -> None:\n",
    "    candidate_text = _read_candidate_text(CANDIDATE_TEXT_PATH)\n",
    "\n",
    "    # You can tweak reasoning_effort/verbosity per run:\n",
    "    assistant_text, reasoning_summary = await _responses_create_with_retry(\n",
    "        instructions=system_prompt,\n",
    "        user_text=candidate_text,\n",
    "        reasoning_effort=\"reasoning_effort\",  # fastest; raise to 'medium'/'high' for tougher tasks\n",
    "        verbosity=\"verbosity\",\n",
    "        max_output_tokens=8000,\n",
    "    )\n",
    "\n",
    "    print(\"=== ASSISTANT OUTPUT ===\")\n",
    "    print(assistant_text.strip())\n",
    "\n",
    "    if reasoning_summary:\n",
    "        print(\"\\n=== REASONING SUMMARY (if provided) ===\")\n",
    "        print(reasoning_summary.strip())\n",
    "\n",
    "\n",
    "# 3) Kick it off (uncomment in notebooks or call from your event loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe14d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5-mini\n",
      "=== ASSISTANT OUTPUT ===\n",
      "{\n",
      "  \"GrandTotal.Amount\": 496.63,\n",
      "  \"Invoice.Date\": \"08.11.2023\",\n",
      "  \"Sender.VatId\": \"ATU78657801\",\n",
      "  \"Vat.Rate\": 20,\n",
      "  \"Net.Amount\": 413.86,\n",
      "  \"Vat.Amount\": 82.77\n",
      "}\n",
      "\n",
      "=== REASONING SUMMARY (if provided) ===\n",
      "**Extracting invoice details**\n",
      "\n",
      "I need to carefully follow the developer's instructions to extract specific invoice details from the text document. I must adhere to the specified JSON schema and not alter any fields. Since the document contains the term “Rechnung” and invoice number “RE073891” with the date “vom 08.11.2023,\" I’ll fill the fields based on the text. The grand total is listed as \"ENDBETRAG: 496,63 EUR,\" so GrandTotal.Amount should be numeric 496.63, considering formatting.\n",
      "**Processing invoice amounts**\n",
      "\n",
      "I need to keep the number of decimal places consistent with the input and ensure not to truncate any numbers. For \"Net.Amount\" and \"Vat.Amount,\" I should return numbers without currency symbols. However, JSON numeric format requires a dot for decimals, so I must convert commas used in the OCR text. The net total is likely represented as \"Warenwert: 413,86 EUR,\" which I should interpret as 413.86. It's crucial to adhere strictly to these formatting rules.\n",
      "**Identifying VAT details**\n",
      "\n",
      "I notice that the text states \"zuüglich 20% MWSt 413,86 82,77 EUR.\" It seems like the net amount is 413.86 and the VAT amount is 82.77. From the text snippet, I spot the net value printed twice, which is a bit odd. The VAT rate should be 20, and it must be recorded without the percentage sign. The seller's VAT ID appears to be \"ATU78657801\" at the bottom. It looks like \"Kunde 21302 / ATU64131246\" is likely the customer's information, not what I need.\n",
      "**Finalizing the extraction**\n",
      "\n",
      "The bottom of the document provides the seller's VAT ID, which is \"ATU78657801.\" I need to use this along with the VAT rate of 20, recorded as a numeric value. The amounts are confirmed: Net.Amount is 413.86, Vat.Amount is 82.77, and GrandTotal.Amount is 496.63. The invoice date is clearly \"08.11.2023.\" I'll ensure I stick to the requirements without making any inferences, as all necessary fields are present. I will focus on maintaining decimal places in the final output.\n",
      "**Preparing JSON output**\n",
      "\n",
      "I need to ensure that I convert commas to dots in the JSON numbers for accuracy. I'll create the JSON object following the schema provided, which includes the keys exactly as specified: \"GrandTotal.Amount,\" \"Invoice.Date,\" \"Sender.VatId,\" \"Vat.Rate,\" \"Net.Amount,\" and \"Vat.Amount.\" The VAT rate will be set as the number 20, with no decimals. I’ll make sure to keep the decimal precision as specified with two decimal places throughout. Now, I’ll produce the final JSON object without any extra text.\n"
     ]
    }
   ],
   "source": [
    "await run_gpt5_on_candidate(reasoning_effort=\"low\", verbosity=\"low\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
