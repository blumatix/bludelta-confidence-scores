{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22c8dcc",
   "metadata": {},
   "source": [
    "# Implementation Roadmap\n",
    "## LLM as a judge\n",
    "### Must-have (ship first)\n",
    "- **Strict JSON schema + retry-on-fail**  \n",
    "  Ensures well-formed, consistent outputs.  \n",
    "\n",
    "- **Multi-trace CoT (k=3–5) + self-confidence**  \n",
    "  Generate multiple reasoning traces; each trace provides its own confidence score.  \n",
    "\n",
    "- **Weighted voting by self-confidence**  \n",
    "  Aggregate values using confidence weights instead of plain majority.  \n",
    "\n",
    "- **Evidence grounding (span linking)**  \n",
    "  Judge must cite exact supporting text spans; missing evidence → lower confidence.  \n",
    "\n",
    "- **Domain validators (hard/soft rules)**  \n",
    "  - Dates follow expected formats  \n",
    "  - VAT ID / IBAN checksum validation  \n",
    "  - `Gross ≈ Net + VAT` (within tolerance)  \n",
    "  - Currency whitelist checks  \n",
    "\n",
    "- **Per-field calibration**  \n",
    "  Map raw confidence to true correctness probability (e.g., isotonic or temperature scaling).  \n",
    "\n",
    "- **Acceptance policy**  \n",
    "  Apply per-field thresholds; for documents, require all critical fields to pass.  \n",
    "\n",
    "- **Auditing & provenance**  \n",
    "  Store chosen value, calibrated confidence, cited spans, validator outcomes, and reasoning traces.  \n",
    "\n",
    "---\n",
    "\n",
    "### High-impact add-ons (still no training)\n",
    "- **Dual-phase reasoning**  \n",
    "  1. Solution reasoning → generate answer  \n",
    "  2. Confidence reasoning → judge and verbalize confidence bin (0–9 or 0–1)  \n",
    "\n",
    "- **Difficulty-aware sampling**  \n",
    "  Use vote dispersion or evidence quality to stop early on easy fields or sample more traces for hard ones.  \n",
    "\n",
    "- **Cross-judge diversity (multiple rubrics)**  \n",
    "  Run the same model with different judging rubrics and combine results:  \n",
    "  - *Format-strict judge*: strict regex/format checks (e.g., dates, VAT IDs)  \n",
    "  - *Arithmetic-strict judge*: check totals and sums (e.g., Gross vs. Net + VAT)  \n",
    "  - *Evidence-strict judge*: require supporting spans near relevant anchors (e.g., “VAT”, “Total”)  \n",
    "\n",
    "- **Few-shot bank (prompt-only)**  \n",
    "  Maintain a curated set of vendor/layout examples; update as new reviewed cases are added.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ops & monitoring\n",
    "- **Coverage vs. accuracy dashboards**  \n",
    "  Track Brier, ECE, NLL, AUROC; monitor auto-accept vs. review rates.  \n",
    "\n",
    "- **Periodic recalibration**  \n",
    "  Refresh calibration mappings as new reviewed cases arrive.  \n",
    "\n",
    "- **Fail-safes**  \n",
    "  If JSON invalid or confidence unstable → deterministic re-ask at temperature=0; else route to review.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052830f",
   "metadata": {},
   "source": [
    "<style>\n",
    "open { color: Red }\n",
    "inprogress { color: Yellow }\n",
    "done { color: Green; text-decoration: line-through }\n",
    "</style>\n",
    "\n",
    "# First steps\n",
    "To start things off, we need to build a simple LLM-as-a-Judge (LaaJ) setup<br>\n",
    "For this we need to do prepare the following:\n",
    "- <done>Access to the Azure hosted GPT5-mini</done>\n",
    "- <inprogress>Access to the data located on the NAS</inprogress>\n",
    "\n",
    "Then we need to force the LLM to create multiple reasoning paths for one input.<br>\n",
    "- <done>Actively encourage the LLM to perform \"slow thinking\" and prepare a json output schema the data needs to fit in.</done><br>\n",
    "- <done>Implement a retry option (with amount of retries as a parameter) to catch LLM errors.</done><br>\n",
    "- <open>Peform validity checks for produced outputs (Valid VAT layout, isNumber for amounts, ...). If these checks fail --> confidence decrease / set to 0.</open><br>\n",
    "- <done>Query the LLM to rate its own output using numeric and written bins (10 bins, [0.0; 0.1], [0.1; 0.2], ...)</done><br>\n",
    "- <inprogress>Implement weighted majority voting based on confidence (Borda voting with exponent)</inprogress><br>\n",
    "\n",
    "<open>Visualize the results of perceived confidence and actual correctness in a bar chart (should be x=y for perfect prediction)</open><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653a805",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0648315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- setup: remove retries, unused vars, fix concurrent name, simpler mkdir ---\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "import logging\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "DOCUMENT_STORAGE = Path(os.getenv(\"DOCUMENT_STORAGE\"))\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "client = AsyncOpenAI(api_key=API_KEY, base_url=ENDPOINT)\n",
    "\n",
    "CONCURRENT_TASKS = 15  # fixed typo\n",
    "\n",
    "logging_path = Path(\"../Logs\")\n",
    "logging_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=f\"{logging_path}/{DEPLOYMENT_NAME}_{TIMESTAMP}.log\",\n",
    "    filemode=\"a\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    force=True,\n",
    ")\n",
    "for noisy in [\"httpx\", \"openai\", \"azure\", \"urllib3\"]:\n",
    "    logging.getLogger(noisy).setLevel(logging.WARNING)\n",
    "\n",
    "with open(\"../Prompts/system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()\n",
    "with open(\"../Prompts/schema.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    schema = json.load(f)\n",
    "formatted_schema = json.dumps(schema, indent=2)\n",
    "system_prompt = system_prompt.replace(\"formatted_schema_representation\", formatted_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff6f84",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26145ee",
   "metadata": {},
   "source": [
    "## Fuzzy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "970ecf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, Optional\n",
    "\n",
    "COMPARE_FIELDS = [\n",
    "    \"Sender.VatId\",\n",
    "    \"GrandTotal.Amount\",\n",
    "    \"Net.Amount\",\n",
    "    \"Vat.Amount\",\n",
    "    \"Vat.Rate\",\n",
    "    \"Invoice.Date\",\n",
    "]\n",
    "\n",
    "NUMERIC_FIELDS = {\n",
    "    \"GrandTotal.Amount\",\n",
    "    \"Net.Amount\",\n",
    "    \"Vat.Amount\",\n",
    "    \"Vat.Rate\",\n",
    "}\n",
    "\n",
    "\n",
    "_TRAILING_RE = re.compile(r\"[ \\t\\r\\n\\.,;:]+$\")  # remove trailing whitespace or punctuation\n",
    "\n",
    "def strip_trailing_punct(x: Any) -> Optional[str]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    return _TRAILING_RE.sub(\"\", s)\n",
    "\n",
    "def parse_number(x: Any) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Improved numeric parsing:\n",
    "    - Rightmost of ',' or '.' is treated as the decimal separator.\n",
    "    \"\"\"\n",
    "    s = strip_trailing_punct(x)\n",
    "    if s is None:\n",
    "        return None\n",
    "\n",
    "    # keep only digits, separators, minus\n",
    "    s = re.sub(r\"[^0-9,.\\-]\", \"\", s)\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    last_dot = s.rfind(\".\")\n",
    "    last_comma = s.rfind(\",\")\n",
    "\n",
    "    if last_dot != -1 and last_comma != -1:\n",
    "        # both present: rightmost decides decimal\n",
    "        if last_comma > last_dot:\n",
    "            # comma is decimal, dots are thousands\n",
    "            s = s.replace(\".\", \"\")\n",
    "            s = s.replace(\",\", \".\")\n",
    "        else:\n",
    "            # dot is decimal, commas are thousands\n",
    "            s = s.replace(\",\", \"\")\n",
    "    elif last_comma != -1:\n",
    "        # only comma present -> decimal comma\n",
    "        s = s.replace(\".\", \"\")  # any stray dots treated as thousands\n",
    "        s = s.replace(\",\", \".\")\n",
    "\n",
    "    try:\n",
    "        val = float(s)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "    return val\n",
    "\n",
    "\n",
    "def normalize_value(field: str, value: Any):\n",
    "    if field in NUMERIC_FIELDS:\n",
    "        return parse_number(value)\n",
    "    # Special-case: for VAT IDs, remove all internal whitespace before compare\n",
    "    if field == \"Sender.VatId\":\n",
    "        s = strip_trailing_punct(value)\n",
    "        if s is None:\n",
    "            return None\n",
    "        return re.sub(r\"\\s+\", \"\", s)\n",
    "    # non-numeric: only remove trailing whitespace/punctuation\n",
    "    return strip_trailing_punct(value)\n",
    "\n",
    "\n",
    "def _round_numeric(val):\n",
    "    if val is None:\n",
    "        return None\n",
    "    return round(float(val), 2)\n",
    "\n",
    "def normalize_for_compare(field: str, value):\n",
    "    \"\"\"Apply trailing-trim for text, number parsing + rounding for numeric. Preserve list shape.\"\"\"\n",
    "    normed = normalize_value(field, value)\n",
    "    if field in NUMERIC_FIELDS:\n",
    "        normed = _round_numeric(normed)\n",
    "    return normed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ab46127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "async def _timed_request(**kwargs):\n",
    "    start = time.time()\n",
    "    resp = await client.responses.create(**kwargs)\n",
    "    runtime = round(time.time() - start, 2)\n",
    "    usage = getattr(resp, \"usage\", None)\n",
    "    return resp, {\n",
    "        \"runtime_sec\": runtime,\n",
    "        \"input_tokens\": getattr(usage, \"input_tokens\", None),\n",
    "        \"output_tokens\": getattr(usage, \"output_tokens\", None),\n",
    "        \"total_tokens\": getattr(usage, \"total_tokens\", None),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "CONFIDENCE_CLASSES = [\n",
    "    \"Almost no chance\",\n",
    "    \"Highly unlikely\",\n",
    "    \"Chances are slight\",\n",
    "    \"Unlikely\",\n",
    "    \"Less than even\",\n",
    "    \"Better than even\",\n",
    "    \"Likely\",\n",
    "    \"Very good chance\",\n",
    "    \"Highly likely\",\n",
    "    \"Almost certain\"\n",
    "]\n",
    "\n",
    "def score_to_verbal(score: float) -> str:\n",
    "    s = 0.0 if score is None else float(score)\n",
    "    s = max(0.0, min(1.0, s))\n",
    "    bounds = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "    idx = max(i for i, b in enumerate(bounds) if s >= b)\n",
    "    return CONFIDENCE_CLASSES[idx]\n",
    "\n",
    "\n",
    "def _extract_reasoning_text(resp) -> str:\n",
    "    chunks = []\n",
    "    for item in getattr(resp, \"output\", []) or []:\n",
    "        if getattr(item, \"type\", \"\") == \"reasoning\":\n",
    "            summary = getattr(item, \"summary\", None)\n",
    "            if isinstance(summary, list):\n",
    "                for seg in summary:\n",
    "                    t = getattr(seg, \"text\", None)\n",
    "                    if t:\n",
    "                        chunks.append(t)\n",
    "            for seg in getattr(item, \"content\", []) or []:\n",
    "                t = getattr(seg, \"text\", None)\n",
    "                if t:\n",
    "                    chunks.append(t)\n",
    "    if chunks:\n",
    "        return \"\\n\".join(chunks).strip()\n",
    "    return getattr(resp, \"output_text\", \"\") or \"\"\n",
    "\n",
    "def _coerce_answer_dict(ans: dict) -> dict:\n",
    "    if isinstance(ans, dict) and \"properties\" in ans and \"values\" in ans:\n",
    "        v = ans.get(\"values\")\n",
    "        return v if isinstance(v, dict) else ans\n",
    "    return ans if isinstance(ans, dict) else {}\n",
    "\n",
    "\n",
    "\n",
    "def _gt_path_for_txt(txt_path):\n",
    "    p = Path(txt_path)\n",
    "    stem_id = p.stem.split(\"_\")[0]  \n",
    "    return sorted(p.parent.glob(f\"{stem_id}_*.json\"))[0]\n",
    "\n",
    "\n",
    "def _norm_scalar(v):\n",
    "    if v is None:\n",
    "        return \"\"\n",
    "    if isinstance(v, (int, float, bool)):\n",
    "        return str(v)\n",
    "    return str(v).strip()\n",
    "\n",
    "\n",
    "def _flatten_gt(gt: dict) -> dict:\n",
    "    out = defaultdict(list)\n",
    "    for node in gt.get(\"DocumentEssentials\", []) or []:\n",
    "        label = node.get(\"Label\")\n",
    "        items = node.get(\"Items\")\n",
    "        if items:\n",
    "            # e.g., \"Vat.Item\" group: collect sub-labels into lists\n",
    "            for sub in items:\n",
    "                sub_label = sub.get(\"Label\")\n",
    "                sub_text = sub.get(\"Text\")\n",
    "                if sub_label:\n",
    "                    out[sub_label].append(_norm_scalar(sub_text))\n",
    "        else:\n",
    "            text = node.get(\"Text\")\n",
    "            if label:\n",
    "                out[label].append(_norm_scalar(text))\n",
    "    # collapse singletons to scalars\n",
    "    flat = {}\n",
    "    for k, vals in out.items():\n",
    "        vals = [v for v in vals if v != \"\"]\n",
    "        flat[k] = vals[0] if len(vals) == 1 else vals\n",
    "    return flat\n",
    "\n",
    "def compare_with_ground_truth(answer_dict: dict, txt_path: str | Path):\n",
    "    gt_path = _gt_path_for_txt(txt_path)\n",
    "    if not Path(gt_path).exists():\n",
    "        return {\n",
    "            \"ground_truth_path\": str(gt_path),\n",
    "            \"per_field\": {},\n",
    "            \"summary\": {\"matched\": 0, \"total\": 0, \"all_match\": False, \"note\": \"ground truth not found\"},\n",
    "        }\n",
    "\n",
    "    gt_json = json.loads(Path(gt_path).read_text(encoding=\"utf-8\"))\n",
    "    gt_flat = _flatten_gt(gt_json)\n",
    "\n",
    "    # Only check the six specified fields\n",
    "    fields_to_check = [f for f in COMPARE_FIELDS if f in gt_flat or f in answer_dict]\n",
    "    per_field = {}\n",
    "    num_matched = 0\n",
    "\n",
    "    for field in fields_to_check:\n",
    "        llm_raw = answer_dict.get(field, None)\n",
    "        gt_raw = gt_flat.get(field, None)\n",
    "\n",
    "        llm_norm = normalize_for_compare(field, llm_raw)\n",
    "        gt_norm = normalize_for_compare(field, gt_raw)\n",
    "\n",
    "        is_match =  llm_norm == gt_norm # values_equal(field, llm_norm, gt_norm)\n",
    "        if is_match:\n",
    "            num_matched += 1\n",
    "\n",
    "        # keep original shapes in output\n",
    "        per_field[field] = [is_match, llm_raw, gt_raw]\n",
    "\n",
    "    total = len(fields_to_check)\n",
    "    return {\n",
    "        \"ground_truth_path\": str(gt_path),\n",
    "        \"per_field\": per_field,\n",
    "        \"summary\": {\"matched\": num_matched, \"total\": total, \"all_match\": num_matched == total},\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acf4a9",
   "metadata": {},
   "source": [
    "# Test Reasoning path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9426837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def _postprocess_final_payload(payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Expecting payload shape:\n",
    "    {\n",
    "      \"answer\": {...},                       # dict conforming to your schema\n",
    "      \"confidence\": {\"field.path\": float}    # 0..1 per field that appears in 'answer'\n",
    "    }\n",
    "    Returns lean record with numeric + verbal per field.\n",
    "    \"\"\"\n",
    "    answer = _coerce_answer_dict(payload.get(\"answer\", {}))\n",
    "    conf_num = payload.get(\"confidence\", {}) or {}\n",
    "\n",
    "    # Build verbal per field for fields present in numeric dict\n",
    "    conf_verbal = {k: score_to_verbal(v) for k, v in conf_num.items()}\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"confidence_numeric\": conf_num,\n",
    "        \"confidence_verbal\": conf_verbal,\n",
    "    }\n",
    "\n",
    "async def run_three_step_invoice_with_reasoning(invoice_txt_path: str, output_dir: Path):\n",
    "    text = Path(invoice_txt_path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Step 1 — SOLUTION REASONING\n",
    "    resp1, log1 = await _timed_request(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        instructions=system_prompt,\n",
    "        input=(\n",
    "            \"Step 1 — SOLUTION REASONING:\\n\"\n",
    "            \"Reason step by step to extract the structured invoice data from the text below. \"\n",
    "            \"Use slow thinking (explore alternatives, verify fields). \"\n",
    "            \"Do NOT output the final answer yet.\\n\\n\"\n",
    "            f\"--- INVOICE TEXT START ---\\n{text}\\n--- INVOICE TEXT END ---\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        max_output_tokens=4000,\n",
    "        text={\"verbosity\": \"low\"},\n",
    "    )\n",
    "    step1_notes = _extract_reasoning_text(resp1)\n",
    "\n",
    "    # Step 2 — CONFIDENCE REASONING (no previous_response_id; pass notes explicitly)\n",
    "    resp2, log2 = await _timed_request(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        instructions=system_prompt,\n",
    "        input=(\n",
    "            \"Step 2 — CONFIDENCE REASONING:\\n\"\n",
    "            \"Evaluate, step by step, how likely each extracted field is correct, based on internal consistency \"\n",
    "            \"and evidence in the text. Do NOT output the final answer yet.\\n\\n\"\n",
    "            f\"--- INVOICE TEXT START ---\\n{text}\\n--- INVOICE TEXT END ---\\n\\n\"\n",
    "            \"--- INTERNAL NOTES FROM STEP 1 (for your reference) ---\\n\"\n",
    "            f\"{step1_notes}\\n\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        text={\"verbosity\": \"low\"},\n",
    "    )\n",
    "    step2_notes = _extract_reasoning_text(resp2)\n",
    "\n",
    "    # Step 3 — FINAL JSON WITH PER-FIELD NUMERIC CONFIDENCE (no previous_response_id; pass both notes)\n",
    "    resp3, log3 = await _timed_request(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        instructions=system_prompt,\n",
    "        input=(\n",
    "            \"Step 3 — FINAL OUTPUT:\\n\"\n",
    "            \"Return ONLY a single valid JSON object with this structure and nothing else:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"answer\": <your final extracted invoice data strictly following this schema: '\n",
    "            f\"{json.dumps(schema)}\"\n",
    "            \">,\\n\"\n",
    "            '  \"confidence\": {\\n'\n",
    "            \"    // For every field you output in 'answer', provide a numeric confidence in [0,1].\\n\"\n",
    "            '    // Use the exact same field keys as in \\\"answer\\\". For nested fields, use dot-notation keys.\\n'\n",
    "            '    // Example: \\\"Net.Amount\\\": 0.82\\n'\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\"\n",
    "            \"Do not include comments in the actual JSON. Do not include any extra keys. No markdown.\\n\\n\"\n",
    "            f\"--- INVOICE TEXT START ---\\n{text}\\n--- INVOICE TEXT END ---\\n\\n\"\n",
    "            \"--- INTERNAL NOTES FROM STEP 1 ---\\n\"\n",
    "            f\"{step1_notes}\\n\\n\"\n",
    "            \"--- INTERNAL NOTES FROM STEP 2 ---\\n\"\n",
    "            f\"{step2_notes}\\n\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        text={\"verbosity\": \"low\"},\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        payload = json.loads(resp3.output_text)\n",
    "    except Exception:\n",
    "        return None  # skip this trace\n",
    "    processed = _postprocess_final_payload(payload)\n",
    "    evaluation = compare_with_ground_truth(processed['answer'], invoice_txt_path)\n",
    "\n",
    "    lean_record = {\n",
    "        \"input_path\": str(invoice_txt_path),\n",
    "        \"answer\": processed[\"answer\"],\n",
    "        \"confidence_numeric\": processed[\"confidence_numeric\"],\n",
    "        \"confidence_verbal\": processed[\"confidence_verbal\"],\n",
    "        \"reasoning\": {\n",
    "            \"step1\": step1_notes,\n",
    "            \"step2\": step2_notes,\n",
    "        },\n",
    "        \"llm_logs\": {\n",
    "            \"step1\": log1,\n",
    "            \"step2\": log2,\n",
    "            \"step3\": log3,\n",
    "        },\n",
    "        \"evaluation\": evaluation,\n",
    "    }\n",
    "\n",
    "    out_path = output_dir / (Path(invoice_txt_path).stem + \".json\")\n",
    "    out_path.write_text(json.dumps(lean_record, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return lean_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51389fd9",
   "metadata": {},
   "source": [
    "# Add weighted majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512691d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrent weighted majority voting with bounded parallelism\n",
    "import asyncio\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def _key(v):\n",
    "    # hashable key for dict/list/float/string values\n",
    "    if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "        return v\n",
    "    return json.dumps(v, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "def _weighted_majority(flat_runs):\n",
    "    consensus_result = {}\n",
    "    debug_info = {}\n",
    "\n",
    "    all_fields = set().union(*[r[\"answer\"].keys() for r in flat_runs])\n",
    "    for field in all_fields:\n",
    "        confidence_sums = defaultdict(float)\n",
    "        appearance_counts = defaultdict(int)\n",
    "\n",
    "        for run in flat_runs:\n",
    "            if field in run[\"answer\"]:\n",
    "                value = run[\"answer\"][field]\n",
    "                confidence = float(run[\"confidence_numeric\"].get(field, 1.0) or 1.0)\n",
    "                key = _key(value)\n",
    "                confidence_sums[key] += confidence\n",
    "                appearance_counts[key] += 1\n",
    "\n",
    "        if confidence_sums:\n",
    "            avg_confidences = {\n",
    "                key: round(confidence_sums[key] / appearance_counts[key], 2)\n",
    "                for key in confidence_sums\n",
    "            }\n",
    "            best_key = max(avg_confidences, key=avg_confidences.get)\n",
    "\n",
    "            try:\n",
    "                best_value = json.loads(best_key)\n",
    "            except Exception:\n",
    "                best_value = best_key\n",
    "\n",
    "            consensus_result[field] = {\n",
    "                \"value\": best_value,\n",
    "                \"avg_confidence\": avg_confidences[best_key],\n",
    "            }\n",
    "            debug_info[field] = {\n",
    "                \"avg_conf_per_value\": avg_confidences,\n",
    "                \"counts\": dict(appearance_counts),\n",
    "            }\n",
    "\n",
    "    return consensus_result, debug_info\n",
    "\n",
    "async def run_self_consistent_invoice(\n",
    "    invoice_txt_path: str,\n",
    "    n_paths: int = 5,\n",
    "    sem: asyncio.Semaphore | None = None,\n",
    "    base_dir: Path | None = None,\n",
    "):\n",
    "    stem = Path(invoice_txt_path).stem                           # e.g., \"274202_grid_concat\"\n",
    "    file_id = stem.split(\"_\")[0]                                 # e.g., \"274202\"\n",
    "\n",
    "    # Use provided base_dir or create a local one (only for standalone usage)\n",
    "    root_out = base_dir or Path(f\"../Output/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "    root_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Per-file subfolder by ID\n",
    "    file_dir = root_out / file_id\n",
    "    file_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    local_sem = sem or asyncio.Semaphore(CONCURRENT_TASKS)\n",
    "\n",
    "    async def _bounded_run(out_dir: Path):\n",
    "        async with local_sem:\n",
    "            return await run_three_step_invoice_with_reasoning(invoice_txt_path, out_dir)\n",
    "\n",
    "    tasks = []\n",
    "    for i in range(n_paths):\n",
    "        out_dir = file_dir / f\"rep_{i+1}\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        tasks.append(asyncio.create_task(_bounded_run(out_dir)))\n",
    "\n",
    "    runs = await asyncio.gather(*tasks)\n",
    "    runs = [r for r in runs if r is not None]  # drop failed traces\n",
    "\n",
    "    flat_runs = [{\"answer\": r[\"answer\"], \"confidence_numeric\": r.get(\"confidence_numeric\", {})} for r in runs]\n",
    "    final_answer, debug = _weighted_majority(flat_runs)\n",
    "\n",
    "    consensus = {\n",
    "        \"input_path\": str(invoice_txt_path),\n",
    "        \"n_paths\": n_paths,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"runs\": runs,\n",
    "        \"vote_debug\": debug,\n",
    "    }\n",
    "    # Write consensus into the ID subfolder\n",
    "    (file_dir / \"consensus.json\").write_text(\n",
    "        json.dumps(consensus, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    return consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8d70e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invoices: 100%|██████████| 2/2 [00:17<00:00,  8.98s/file]\n"
     ]
    }
   ],
   "source": [
    "# Progress bar for folder runner (async-friendly)\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm.auto import tqdm  # pip install tqdm\n",
    "\n",
    "async def run_invoices_in_folder(\n",
    "    folder_path: str | Path,\n",
    "    n_paths: int = 5,\n",
    "    max_files: Optional[int] = None,\n",
    "):\n",
    "    folder = Path(folder_path)\n",
    "    assert folder.is_dir(), f\"Not a folder: {folder}\"\n",
    "\n",
    "    files = sorted(folder.rglob(\"*_grid_concat.txt\"))\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    run_dir = Path(f\"../Output/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    shared_sem = asyncio.Semaphore(CONCURRENT_TASKS)\n",
    "\n",
    "    async def _run_file(txt_path: Path):\n",
    "        return str(txt_path), await run_self_consistent_invoice(\n",
    "            str(txt_path),\n",
    "            n_paths=n_paths,\n",
    "            sem=shared_sem,\n",
    "            base_dir=run_dir,\n",
    "        )\n",
    "\n",
    "    results = {}\n",
    "    coros = [_run_file(p) for p in files]\n",
    "    with tqdm(total=len(coros), desc=\"Invoices\", unit=\"file\") as pbar:\n",
    "        for fut in asyncio.as_completed(coros):\n",
    "            p, consensus = await fut\n",
    "            results[p] = consensus\n",
    "            pbar.update(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example:\n",
    "results = await run_invoices_in_folder(\"../Documents/273699\", n_paths=1, max_files=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3a9f9",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f210aeb",
   "metadata": {},
   "source": [
    "## Individual Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c16b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _make_bins(min_conf: float = 0.90, max_conf: float = 1.00, bin_width: float = 0.01):\n",
    "    min_conf = float(min_conf); max_conf = float(max_conf); bin_width = float(bin_width)\n",
    "    assert 0.0 <= min_conf < max_conf <= 1.0, \"Range must be within [0,1] and min<max\"\n",
    "    assert bin_width > 0, \"bin_width must be > 0\"\n",
    "    # Left edges; ensure we include max bin by not stepping past max_conf\n",
    "    edges = np.arange(min_conf, max_conf, bin_width)\n",
    "    # If max_conf is exactly on a step, last edge < max_conf; we want last bin to include max_conf\n",
    "    num_bins = len(edges)\n",
    "    centers = edges + bin_width / 2.0\n",
    "    # Clip the last center to max_conf if it goes beyond\n",
    "    if centers.size > 0:\n",
    "        centers[-1] = min(centers[-1], max_conf)\n",
    "    labels = [f\"{c:.2f}\" for c in centers]\n",
    "    # If the last bin should explicitly read max_conf (e.g., 1.00), set label\n",
    "    if labels:\n",
    "        labels[-1] = f\"{max_conf:.2f}\"\n",
    "    return edges, centers, labels, num_bins, bin_width, min_conf, max_conf\n",
    "\n",
    "def _bin_index_conf(c: float, edges: np.ndarray, min_conf: float, max_conf: float, bin_width: float, num_bins: int):\n",
    "    if c is None:\n",
    "        return None\n",
    "    c = max(0.0, min(1.0, float(c)))\n",
    "    if c < min_conf:\n",
    "        return None\n",
    "    if c >= max_conf:\n",
    "        return num_bins - 1  # include max_conf in the last bin\n",
    "    return int(math.floor((c - min_conf) / bin_width))\n",
    "\n",
    "def plot_calibration_for_run(run_dir: str | Path, min_conf: float = 0.90, max_conf: float = 1.00, bin_width: float = 0.01):\n",
    "    run_dir = Path(run_dir)\n",
    "    assert run_dir.is_dir(), f\"Not a directory: {run_dir}\"\n",
    "\n",
    "    consensus_files = sorted(run_dir.glob(\"*/consensus.json\"))\n",
    "    assert consensus_files, f\"No consensus.json files under {run_dir}\"\n",
    "\n",
    "    edges, centers, labels, num_bins, bw, minc, maxc = _make_bins(min_conf, max_conf, bin_width)\n",
    "    stats = defaultdict(lambda: [ {\"correct\": 0, \"total\": 0} for _ in range(num_bins) ])\n",
    "\n",
    "    for cf in consensus_files:\n",
    "        data = json.loads(cf.read_text(encoding=\"utf-8\"))\n",
    "        input_path = data.get(\"input_path\")\n",
    "        final_answer = data.get(\"final_answer\", {}) or {}\n",
    "        answer_values, per_field_conf = {}, {}\n",
    "        for field, entry in final_answer.items():\n",
    "            if isinstance(entry, dict) and \"value\" in entry:\n",
    "                answer_values[field] = entry.get(\"value\")\n",
    "                per_field_conf[field] = entry.get(\"avg_confidence\")\n",
    "            else:\n",
    "                answer_values[field] = entry\n",
    "                per_field_conf[field] = None\n",
    "        evaluation = compare_with_ground_truth(answer_values, input_path)\n",
    "        for field, triple in (evaluation.get(\"per_field\", {}) or {}).items():\n",
    "            try:\n",
    "                is_match = bool(triple[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "            idx = _bin_index_conf(per_field_conf.get(field), edges, minc, maxc, bw, num_bins)\n",
    "            if idx is None:\n",
    "                continue\n",
    "            stats[field][idx][\"total\"] += 1\n",
    "            stats[field][idx][\"correct\"] += 1 if is_match else 0\n",
    "\n",
    "    for field, bin_counts in stats.items():\n",
    "        x_pos = centers\n",
    "        acc_pct = []\n",
    "        x_conf, y_acc = [], []\n",
    "        for i in range(num_bins):\n",
    "            c = bin_counts[i]\n",
    "            if c[\"total\"] > 0:\n",
    "                acc = 100.0 * c[\"correct\"] / c[\"total\"]\n",
    "                acc_pct.append(acc)\n",
    "                x_conf.append(float(x_pos[i]))\n",
    "                y_acc.append(acc)\n",
    "            else:\n",
    "                acc_pct.append(0.0)\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(x_pos, acc_pct, width=bw * 0.9, color=\"#4C78A8\", alpha=0.8, label=\"Bin accuracy (consensus)\")\n",
    "\n",
    "        if len(x_conf) >= 2:\n",
    "            m, b0 = np.polyfit(np.array(x_conf), np.array(y_acc), 1)\n",
    "            x_line = np.linspace(minc, maxc, 100)\n",
    "            y_line = m * x_line + b0\n",
    "            plt.plot(x_line, y_line, color=\"#E45756\", linewidth=2, label=\"Linear fit\")\n",
    "\n",
    "        x_ref = np.linspace(minc, maxc, 100)\n",
    "        y_ref = 100.0 * x_ref\n",
    "        plt.plot(x_ref, y_ref, color=\"#72B7B2\", linestyle=\"--\", linewidth=1.5, label=\"Ideal\")\n",
    "\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlabel(f\"Model confidence (bins, {int(bw*100)}% from {minc:.2f} to {maxc:.2f})\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title(f\"Calibration — {field}\")\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "        plt.xticks(x_pos, labels)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        out_path = run_dir / f\"calibration_{field.replace('.', '_')}_{int(minc*100)}_{int(maxc*100)}_{int(bw*100)}pct.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {out_path}\")\n",
    "\n",
    "def plot_calibration_for_individual_runs(run_dir: str | Path, min_conf: float = 0.90, max_conf: float = 1.00, bin_width: float = 0.01):\n",
    "    run_dir = Path(run_dir)\n",
    "    assert run_dir.is_dir(), f\"Not a directory: {run_dir}\"\n",
    "\n",
    "    run_files = sorted(run_dir.glob(\"*/*/*.json\"))\n",
    "    if not run_files:\n",
    "        print(f\"No individual run files under {run_dir}\")\n",
    "        return\n",
    "\n",
    "    edges, centers, labels, num_bins, bw, minc, maxc = _make_bins(min_conf, max_conf, bin_width)\n",
    "    stats = defaultdict(lambda: [ {\"correct\": 0, \"total\": 0} for _ in range(num_bins) ])\n",
    "\n",
    "    for rf in run_files:\n",
    "        try:\n",
    "            data = json.loads(rf.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            continue\n",
    "        input_path = data.get(\"input_path\")\n",
    "        answer = data.get(\"answer\", {}) or {}\n",
    "        conf = data.get(\"confidence_numeric\", {}) or {}\n",
    "        evaluation = compare_with_ground_truth(answer, input_path)\n",
    "        for field, triple in (evaluation.get(\"per_field\", {}) or {}).items():\n",
    "            try:\n",
    "                is_match = bool(triple[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "            idx = _bin_index_conf(conf.get(field), edges, minc, maxc, bw, num_bins)\n",
    "            if idx is None:\n",
    "                continue\n",
    "            stats[field][idx][\"total\"] += 1\n",
    "            stats[field][idx][\"correct\"] += 1 if is_match else 0\n",
    "\n",
    "    for field, bin_counts in stats.items():\n",
    "        x_pos = centers\n",
    "        acc_pct = []\n",
    "        x_conf, y_acc = [], []\n",
    "        for i in range(num_bins):\n",
    "            c = bin_counts[i]\n",
    "            if c[\"total\"] > 0:\n",
    "                acc = 100.0 * c[\"correct\"] / c[\"total\"]\n",
    "                acc_pct.append(acc)\n",
    "                x_conf.append(float(x_pos[i]))\n",
    "                y_acc.append(acc)\n",
    "            else:\n",
    "                acc_pct.append(0.0)\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(x_pos, acc_pct, width=bw * 0.9, color=\"#4C78A8\", alpha=0.8, label=\"Bin accuracy (runs)\")\n",
    "\n",
    "        if len(x_conf) >= 2:\n",
    "            m, b0 = np.polyfit(np.array(x_conf), np.array(y_acc), 1)\n",
    "            x_line = np.linspace(minc, maxc, 100)\n",
    "            y_line = m * x_line + b0\n",
    "            plt.plot(x_line, y_line, color=\"#E45756\", linewidth=2, label=\"Linear fit\")\n",
    "\n",
    "        x_ref = np.linspace(minc, maxc, 100)\n",
    "        y_ref = 100.0 * x_ref\n",
    "        plt.plot(x_ref, y_ref, color=\"#72B7B2\", linestyle=\"--\", linewidth=1.5, label=\"Ideal\")\n",
    "\n",
    "        plt.ylim(0, 100)\n",
    "        plt.xlabel(f\"Model confidence (bins, {int(bw*100)}% from {minc:.2f} to {maxc:.2f})\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.title(f\"Calibration (individual runs) — {field}\")\n",
    "        plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "        plt.xticks(x_pos, labels)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        out_path = run_dir / f\"calibration_runs_{field.replace('.', '_')}_{int(minc*100)}_{int(maxc*100)}_{int(bw*100)}pct.png\"\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.show()\n",
    "        print(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9165ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80–100% in 2% bins\n",
    "FOLDER_TO_VISUALIZE = \"2025-09-12_09-19-57\"\n",
    "plot_calibration_for_individual_runs(f\"../Output/{FOLDER_TO_VISUALIZE}\", min_conf=0.80, max_conf=1.00, bin_width=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdef631",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_for_run(f\"../Output/{FOLDER_TO_VISUALIZE}\", min_conf=0.80, max_conf=1.00, bin_width=0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a4fcf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== File 273560 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 43349701\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 273567 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 31714302\n",
      "  confidence: 0.99\n",
      "\n",
      "=== File 273595 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 43349701\n",
      "  confidence: 0.97\n",
      "\n",
      "=== File 273607 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU14391601,\n",
      "  confidence: 0.98\n",
      "- Net.Amount\n",
      "  predicted: 73548.0\n",
      "  ground_tr: ['73.548,00', '73.548,00']\n",
      "  confidence: 0.96\n",
      "- Vat.Amount\n",
      "  predicted: 7354.8\n",
      "  ground_tr: ['7.354,80', '7.354,80']\n",
      "  confidence: 0.96\n",
      "- Vat.Rate\n",
      "  predicted: 10\n",
      "  ground_tr: ['10', '10']\n",
      "  confidence: 0.97\n",
      "\n",
      "=== File 273628 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU14391601,\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 273642 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 31714302\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 273664 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU65980977\n",
      "  ground_tr: ATU 65980977\n",
      "  confidence: 0.9\n",
      "\n",
      "=== File 273684 ===\n",
      "- Net.Amount\n",
      "  predicted: 20.9\n",
      "  ground_tr: ['20,90', '20,90']\n",
      "  confidence: 0.95\n",
      "- Vat.Amount\n",
      "  predicted: 4.18\n",
      "  ground_tr: ['4,18', '4,18']\n",
      "  confidence: 0.95\n",
      "- Vat.Rate\n",
      "  predicted: 20\n",
      "  ground_tr: ['20.00', '20,00']\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 273688 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU65980977\n",
      "  ground_tr: ATU 65980977\n",
      "  confidence: 0.93\n",
      "- Invoice.Date\n",
      "  predicted: 2. Nov 2023\n",
      "  ground_tr: 23.10.2023\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 273693 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: 102.64\n",
      "  ground_tr: 106,32\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 273918 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU26011605,\n",
      "  confidence: 0.95\n",
      "- Invoice.Date\n",
      "  predicted: 25.01.2024\n",
      "  ground_tr: 26.01.2024\n",
      "  confidence: 0.87\n",
      "\n",
      "=== File 273939 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU52106509\n",
      "  ground_tr: ATU 52106509\n",
      "  confidence: 0.96\n",
      "\n",
      "=== File 274153 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU14391601,\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274154 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU66213527,\n",
      "  confidence: 0.92\n",
      "\n",
      "=== File 274156 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU35980300\n",
      "  ground_tr: ATU 35980300\n",
      "  confidence: 0.87\n",
      "\n",
      "=== File 274157 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU66213527,\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274159 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU65960748\n",
      "  confidence: 0.99\n",
      "\n",
      "=== File 274160 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU52919604\n",
      "  ground_tr: ATU 52919604,\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 274166 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU14391601,\n",
      "  confidence: 0.91\n",
      "\n",
      "=== File 274175 ===\n",
      "- Invoice.Date\n",
      "  predicted: 26.01.2024\n",
      "  ground_tr: 29.01.2024\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274177 ===\n",
      "- Net.Amount\n",
      "  predicted: 1169.14\n",
      "  ground_tr: ['665,10', '504,04']\n",
      "  confidence: 0.95\n",
      "- Vat.Amount\n",
      "  predicted: 183.42\n",
      "  ground_tr: ['133,02', '50,40']\n",
      "  confidence: 0.94\n",
      "- Vat.Rate\n",
      "  predicted: \n",
      "  ground_tr: ['20', '10']\n",
      "  confidence: 0.3\n",
      "\n",
      "=== File 274183 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU35980300\n",
      "  ground_tr: ATU 35980300\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274187 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU52919604\n",
      "  ground_tr: ATU 52919604,\n",
      "  confidence: 0.96\n",
      "\n",
      "=== File 274188 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 43349701\n",
      "  confidence: 0.96\n",
      "\n",
      "=== File 274201 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU79411414\n",
      "  confidence: 0.95\n",
      "- Net.Amount\n",
      "  predicted: 736.42\n",
      "  ground_tr: ['562,90', '173,52']\n",
      "  confidence: 0.93\n",
      "- Vat.Amount\n",
      "  predicted: 90.99\n",
      "  ground_tr: ['56,29', '34,70']\n",
      "  confidence: 0.87\n",
      "- Vat.Rate\n",
      "  predicted: \n",
      "  ground_tr: ['10', '20']\n",
      "  confidence: 0.77\n",
      "\n",
      "=== File 274202 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 43349701\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 274205 ===\n",
      "- Net.Amount\n",
      "  predicted: 6021.17\n",
      "  ground_tr: ['6.021,17', '6.021,17']\n",
      "  confidence: 0.91\n",
      "- Vat.Amount\n",
      "  predicted: 1204.23\n",
      "  ground_tr: ['1.204,23', '1.204,23']\n",
      "  confidence: 0.92\n",
      "- Vat.Rate\n",
      "  predicted: 20\n",
      "  ground_tr: ['20', '20']\n",
      "  confidence: 0.92\n",
      "- Invoice.Date\n",
      "  predicted: 01.2024\n",
      "  ground_tr: 01.01.2023\n",
      "  confidence: 0.75\n",
      "\n",
      "=== File 274243 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: 64.41\n",
      "  ground_tr: 67,09\n",
      "  confidence: 0.95\n",
      "- Net.Amount\n",
      "  predicted: 55.91\n",
      "  ground_tr: ['55,91', '55,91']\n",
      "  confidence: 0.95\n",
      "- Vat.Amount\n",
      "  predicted: 11.18\n",
      "  ground_tr: ['11,18', '11,18']\n",
      "  confidence: 0.95\n",
      "- Vat.Rate\n",
      "  predicted: 20.0\n",
      "  ground_tr: ['20.00', '20,00']\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 274244 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: 47.16\n",
      "  ground_tr: 48,52\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 274298 ===\n",
      "- Net.Amount\n",
      "  predicted: 2953.2\n",
      "  ground_tr: ['2.219,37', '733,83']\n",
      "  confidence: 0.94\n",
      "- Vat.Amount\n",
      "  predicted: 221.95\n",
      "  ground_tr: ['221,95', '0,00']\n",
      "  confidence: 0.85\n",
      "- Vat.Rate\n",
      "  predicted: 10\n",
      "  ground_tr: ['10,00', '0,00']\n",
      "  confidence: 0.78\n",
      "\n",
      "=== File 274299 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU34011805\n",
      "  ground_tr: ATU38173800\n",
      "  confidence: 0.95\n",
      "- Net.Amount\n",
      "  predicted: 3620.71\n",
      "  ground_tr: 2.706,68\n",
      "  confidence: 0.9\n",
      "\n",
      "=== File 274371 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU37102103\n",
      "  ground_tr: ATU 38297008\n",
      "  confidence: 0.95\n",
      "- Net.Amount\n",
      "  predicted: 750.19\n",
      "  ground_tr: 179,99\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274386 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU38297008\n",
      "  ground_tr: ATU 38297008\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274395 ===\n",
      "- Net.Amount\n",
      "  predicted: 460.67\n",
      "  ground_tr: ['309, 05', '151, 62']\n",
      "  confidence: 0.91\n",
      "- Vat.Amount\n",
      "  predicted: 46.06\n",
      "  ground_tr: ['30, 90', '15,16']\n",
      "  confidence: 0.92\n",
      "- Vat.Rate\n",
      "  predicted: 10\n",
      "  ground_tr: ['10', '10']\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274400 ===\n",
      "- Net.Amount\n",
      "  predicted: 1187.65\n",
      "  ground_tr: 860,13\n",
      "  confidence: 0.91\n",
      "- Vat.Amount\n",
      "  predicted: 91.87\n",
      "  ground_tr: 86,01\n",
      "  confidence: 0.91\n",
      "\n",
      "=== File 274686 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU66213527,\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 274702 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 22684307\n",
      "  confidence: 0.9\n",
      "\n",
      "=== File 274708 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU66213527,\n",
      "  confidence: 0.96\n",
      "\n",
      "=== File 274718 ===\n",
      "- Net.Amount\n",
      "  predicted: 7967.81\n",
      "  ground_tr: ['4.365,41', '3.602,4']\n",
      "  confidence: 0.97\n",
      "- Vat.Amount\n",
      "  predicted: 1157.02\n",
      "  ground_tr: ['436,54', '720,48']\n",
      "  confidence: 0.97\n",
      "- Vat.Rate\n",
      "  predicted: \n",
      "  ground_tr: ['10', '20']\n",
      "  confidence: 0.78\n",
      "\n",
      "=== File 274720 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU18804300\n",
      "  ground_tr: ATU 18804300\n",
      "  confidence: 0.96\n",
      "\n",
      "=== File 274726 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22684307\n",
      "  ground_tr: ATU 22684307\n",
      "  confidence: 0.88\n",
      "\n",
      "=== File 274757 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU64424288\n",
      "  ground_tr: ATU 64424288\n",
      "  confidence: 0.97\n",
      "\n",
      "=== File 274797 ===\n",
      "- Net.Amount\n",
      "  predicted: 77.85\n",
      "  ground_tr: ['77,85', '77,85']\n",
      "  confidence: 0.96\n",
      "- Vat.Amount\n",
      "  predicted: 15.57\n",
      "  ground_tr: ['15,57', '15,57']\n",
      "  confidence: 0.94\n",
      "- Vat.Rate\n",
      "  predicted: 20.0\n",
      "  ground_tr: ['20.00', '20,00']\n",
      "  confidence: 0.94\n",
      "\n",
      "=== File 274800 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: 473.98\n",
      "  ground_tr: 493,13\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 274811 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: \n",
      "  ground_tr: 542,08\n",
      "  confidence: 1.0\n",
      "\n",
      "=== File 274819 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: 3174.64\n",
      "  ground_tr: 3.174,64 -\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274828 ===\n",
      "- Sender.VatId\n",
      "  predicted: DE250202095\n",
      "  ground_tr: DE 159518412\n",
      "  confidence: 0.85\n",
      "\n",
      "=== File 274832 ===\n",
      "- Sender.VatId\n",
      "  predicted: DE250202095\n",
      "  ground_tr: DE 159518412\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 274928 ===\n",
      "- GrandTotal.Amount\n",
      "  predicted: 57985.12\n",
      "  ground_tr: 100,00\n",
      "  confidence: 0.97\n",
      "\n",
      "=== File 275073 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU66643000,\n",
      "  confidence: 0.9\n",
      "\n",
      "=== File 275075 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU14391601,\n",
      "  confidence: 0.94\n",
      "\n",
      "=== File 275084 ===\n",
      "- Net.Amount\n",
      "  predicted: 20646.53\n",
      "  ground_tr: ['68,58', '20.577,95']\n",
      "  confidence: 0.94\n",
      "- Vat.Amount\n",
      "  predicted: 2057.8\n",
      "  ground_tr: ['13,72', '2.057,80']\n",
      "  confidence: 0.94\n",
      "- Vat.Rate\n",
      "  predicted: 10\n",
      "  ground_tr: ['20', '10']\n",
      "  confidence: 0.88\n",
      "\n",
      "=== File 275095 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU38510009\n",
      "  confidence: 0.95\n",
      "- Net.Amount\n",
      "  predicted: 1489.73\n",
      "  ground_tr: ['130,10', '1.359,63']\n",
      "  confidence: 0.9\n",
      "- Vat.Amount\n",
      "  predicted: 161.98\n",
      "  ground_tr: ['26,02', '135,96']\n",
      "  confidence: 0.92\n",
      "\n",
      "=== File 275096 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU66643000,\n",
      "  confidence: 0.95\n",
      "\n",
      "=== File 275104 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU 43349701\n",
      "  confidence: 0.94\n",
      "- Invoice.Date\n",
      "  predicted: 31-01-2024\n",
      "  ground_tr: 29-01-2024\n",
      "  confidence: 0.96\n",
      "\n",
      "=== File 275138 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU 22642807\n",
      "  ground_tr: ATU38510009\n",
      "  confidence: 0.93\n",
      "\n",
      "=== File 275144 ===\n",
      "- Sender.VatId\n",
      "  predicted: ATU22642807\n",
      "  ground_tr: ATU14391601,\n",
      "  confidence: 0.9\n",
      "\n",
      "Scanned: 150 files | Files with errors: 57 | Total wrong fields: 91\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def print_wrong_consensus(run_dir: str | Path, only_fields: list[str] | None = None):\n",
    "    run_dir = Path(run_dir)\n",
    "    assert run_dir.is_dir(), f\"Not a directory: {run_dir}\"\n",
    "\n",
    "    consensus_files = sorted(run_dir.glob(\"*/consensus.json\"))\n",
    "    if not consensus_files:\n",
    "        print(f\"No consensus.json files under {run_dir}\")\n",
    "        return\n",
    "\n",
    "    total_files = 0\n",
    "    files_with_errors = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    for cf in consensus_files:\n",
    "        data = json.loads(cf.read_text(encoding=\"utf-8\"))\n",
    "        input_path = data.get(\"input_path\")\n",
    "        final_answer = data.get(\"final_answer\", {}) or {}\n",
    "\n",
    "        # Build flat answer dict and per-field confidence from consensus\n",
    "        answer_values = {}\n",
    "        per_field_conf = {}\n",
    "        for field, entry in final_answer.items():\n",
    "            if only_fields and field not in only_fields:\n",
    "                continue\n",
    "            if isinstance(entry, dict) and \"value\" in entry:\n",
    "                answer_values[field] = entry.get(\"value\")\n",
    "                per_field_conf[field] = entry.get(\"avg_confidence\")\n",
    "            else:\n",
    "                answer_values[field] = entry\n",
    "                per_field_conf[field] = None\n",
    "\n",
    "        total_files += 1\n",
    "\n",
    "        # Evaluate vs. ground truth\n",
    "        evaluation = compare_with_ground_truth(answer_values, input_path)\n",
    "        per_field_eval = evaluation.get(\"per_field\", {})\n",
    "        if not per_field_eval:\n",
    "            # ground truth missing or no comparable fields\n",
    "            continue\n",
    "\n",
    "        wrongs = []\n",
    "        for field, triple in per_field_eval.items():\n",
    "            try:\n",
    "                is_match = bool(triple[0])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if not is_match:\n",
    "                pred_val = answer_values.get(field)\n",
    "                gt_val = triple[2]\n",
    "                conf = per_field_conf.get(field)\n",
    "                wrongs.append((field, pred_val, gt_val, conf))\n",
    "\n",
    "        if wrongs:\n",
    "            files_with_errors += 1\n",
    "            file_id = Path(input_path).stem.split(\"_\")[0] if input_path else cf.parent.name\n",
    "            print(f\"\\n=== File {file_id} ===\")\n",
    "            for field, pred, gt, conf in wrongs:\n",
    "                total_errors += 1\n",
    "                print(f\"- {field}\")\n",
    "                print(f\"  predicted: {pred}\")\n",
    "                print(f\"  ground_tr: {gt}\")\n",
    "                print(f\"  confidence: {conf}\")\n",
    "\n",
    "    print(f\"\\nScanned: {total_files} files | Files with errors: {files_with_errors} | Total wrong fields: {total_errors}\")\n",
    "\n",
    "# Example:\n",
    "print_wrong_consensus(f'../Output/{FOLDER_TO_VISUALIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "982d697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATU65980977 ATU65980977\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_raw = \"ATU65980977\"\n",
    "gt_raw = \"ATU 65980977\"\n",
    "llm_norm = normalize_for_compare(\"Sender.VatId\", llm_raw)\n",
    "gt_norm = normalize_for_compare(\"Sender.VatId\", gt_raw)\n",
    "\n",
    "print(f\"{llm_norm} {gt_norm}\")\n",
    "\n",
    "is_match =  llm_norm == gt_norm # \n",
    "print(is_match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
