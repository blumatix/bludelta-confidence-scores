{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22c8dcc",
   "metadata": {},
   "source": [
    "# Implementation Roadmap\n",
    "## LLM as a judge\n",
    "### Must-have (ship first)\n",
    "- **Strict JSON schema + retry-on-fail**  \n",
    "  Ensures well-formed, consistent outputs.  \n",
    "\n",
    "- **Multi-trace CoT (k=3–5) + self-confidence**  \n",
    "  Generate multiple reasoning traces; each trace provides its own confidence score.  \n",
    "\n",
    "- **Weighted voting by self-confidence**  \n",
    "  Aggregate values using confidence weights instead of plain majority.  \n",
    "\n",
    "- **Evidence grounding (span linking)**  \n",
    "  Judge must cite exact supporting text spans; missing evidence → lower confidence.  \n",
    "\n",
    "- **Domain validators (hard/soft rules)**  \n",
    "  - Dates follow expected formats  \n",
    "  - VAT ID / IBAN checksum validation  \n",
    "  - `Gross ≈ Net + VAT` (within tolerance)  \n",
    "  - Currency whitelist checks  \n",
    "\n",
    "- **Per-field calibration**  \n",
    "  Map raw confidence to true correctness probability (e.g., isotonic or temperature scaling).  \n",
    "\n",
    "- **Acceptance policy**  \n",
    "  Apply per-field thresholds; for documents, require all critical fields to pass.  \n",
    "\n",
    "- **Auditing & provenance**  \n",
    "  Store chosen value, calibrated confidence, cited spans, validator outcomes, and reasoning traces.  \n",
    "\n",
    "---\n",
    "\n",
    "### High-impact add-ons (still no training)\n",
    "- **Dual-phase reasoning**  \n",
    "  1. Solution reasoning → generate answer  \n",
    "  2. Confidence reasoning → judge and verbalize confidence bin (0–9 or 0–1)  \n",
    "\n",
    "- **Difficulty-aware sampling**  \n",
    "  Use vote dispersion or evidence quality to stop early on easy fields or sample more traces for hard ones.  \n",
    "\n",
    "- **Cross-judge diversity (multiple rubrics)**  \n",
    "  Run the same model with different judging rubrics and combine results:  \n",
    "  - *Format-strict judge*: strict regex/format checks (e.g., dates, VAT IDs)  \n",
    "  - *Arithmetic-strict judge*: check totals and sums (e.g., Gross vs. Net + VAT)  \n",
    "  - *Evidence-strict judge*: require supporting spans near relevant anchors (e.g., “VAT”, “Total”)  \n",
    "\n",
    "- **Few-shot bank (prompt-only)**  \n",
    "  Maintain a curated set of vendor/layout examples; update as new reviewed cases are added.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ops & monitoring\n",
    "- **Coverage vs. accuracy dashboards**  \n",
    "  Track Brier, ECE, NLL, AUROC; monitor auto-accept vs. review rates.  \n",
    "\n",
    "- **Periodic recalibration**  \n",
    "  Refresh calibration mappings as new reviewed cases arrive.  \n",
    "\n",
    "- **Fail-safes**  \n",
    "  If JSON invalid or confidence unstable → deterministic re-ask at temperature=0; else route to review.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052830f",
   "metadata": {},
   "source": [
    "<style>\n",
    "open { color: Red }\n",
    "inprogress { color: Yellow }\n",
    "done { color: Green; text-decoration: line-through }\n",
    "</style>\n",
    "\n",
    "# First steps\n",
    "To start things off, we need to build a simple LLM-as-a-Judge (LaaJ) setup<br>\n",
    "For this we need to do prepare the following:\n",
    "- <done>Access to the Azure hosted GPT5-mini</done>\n",
    "- <inprogress>Access to the data located on the NAS</inprogress>\n",
    "\n",
    "Then we need to force the LLM to create multiple reasoning paths for one input.<br>\n",
    "- <done>Actively encourage the LLM to perform \"slow thinking\" and prepare a json output schema the data needs to fit in.</done><br>\n",
    "- <done>Implement a retry option (with amount of retries as a parameter) to catch LLM errors.</done><br>\n",
    "- <open>Peform validity checks for produced outputs (Valid VAT layout, isNumber for amounts, ...). If these checks fail --> confidence decrease / set to 0.</open><br>\n",
    "- <open>Query the LLM to rate its own output using numeric and written bins (10 bins, [0.0; 0.1], [0.1; 0.2], ...)</open><br>\n",
    "- <open>Implement weighted majority voting based on confidence (Borda voting with exponent)</open><br>\n",
    "\n",
    "<open>Visualize the results of perceived confidence and actual correctness in a bar chart (should be x=y for perfect prediction)</open><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653a805",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0648315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from openai import AsyncOpenAI, RateLimitError\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv(override = True)\n",
    "# ---------------------------------HELPER-VARS--------------------------------\n",
    "DOCUMENT_STORAGE = Path(os.getenv(\"DOCUMENT_STORAGE\"))\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# ---------------------------------LLM-CONFIG---------------------------------\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# ---------------------------------LLM-SETUP---------------------------------\n",
    "client = AsyncOpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=ENDPOINT\n",
    ")\n",
    "\n",
    "CONCCURRENT_TASKS = 15\n",
    "REQUEST_DELAY = 1 # seconds\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BACKOFF = 5  # seconds\n",
    "\n",
    "# Whitelist of retryable exceptions\n",
    "RETRYABLE_EXCEPTIONS = (\n",
    "    RateLimitError,\n",
    "    httpx.ConnectTimeout,\n",
    "    httpx.ReadTimeout,\n",
    "    httpx.HTTPStatusError,\n",
    "    httpx.RemoteProtocolError,\n",
    "    httpx.NetworkError,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------LOGGING---------------------------------\n",
    "logging_path = Path(\"../Logs\")\n",
    "if not logging_path.exists():\n",
    "    logging_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=f\"{logging_path}/{DEPLOYMENT_NAME}_{TIMESTAMP}.log\",\n",
    "    filemode=\"a\",                     \n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    force=True\n",
    ")\n",
    "\n",
    "for noisy in [\"httpx\", \"openai\", \"azure\", \"urllib3\"]:\n",
    "    logging.getLogger(noisy).setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Load system prompt\n",
    "with open(\"../Prompts/system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "# Load the schema from file\n",
    "with open(\"../Prompts/schema.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "# Turn it into pretty JSON for prompt injection\n",
    "formatted_schema = json.dumps(schema, indent=2)\n",
    "\n",
    "system_prompt = system_prompt.replace(\"formatted_schema_representation\",formatted_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acf4a9",
   "metadata": {},
   "source": [
    "# Test Reasoning path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9426837d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unknown parameter: ''.\", 'type': 'invalid_request_error', 'param': None, 'code': 'unknown_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 409\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# ===== Examples =====\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[38;5;66;03m# Single-pass for many files:\u001b[39;00m\n\u001b[32m    408\u001b[39m paths = [\u001b[33m\"\u001b[39m\u001b[33m../Documents/273366/273277_page1_grid.txt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m out = \u001b[38;5;28;01mawait\u001b[39;00m run_many_invoices(paths)\n\u001b[32m    411\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m\"\u001b[39m\u001b[33mintermediate done\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# Self-consistency for one file:\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mrun_many_invoices\u001b[39m\u001b[34m(invoice_txt_paths)\u001b[39m\n\u001b[32m    184\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_three_step_invoice_with_reasoning(p, output_dir)\n\u001b[32m    186\u001b[39m tasks = [asyncio.create_task(_bounded(p)) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m invoice_txt_paths]\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    189\u001b[39m (output_dir / \u001b[33m\"\u001b[39m\u001b[33m_index.json\u001b[39m\u001b[33m\"\u001b[39m).write_text(json.dumps(results, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m, indent=\u001b[32m2\u001b[39m), encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33moutput_dir\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(output_dir), \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: results}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mrun_many_invoices.<locals>._bounded\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_bounded\u001b[39m(p):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sem:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_three_step_invoice_with_reasoning(p, output_dir)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mrun_three_step_invoice_with_reasoning\u001b[39m\u001b[34m(invoice_txt_path, output_dir)\u001b[39m\n\u001b[32m     96\u001b[39m text = Path(invoice_txt_path).read_text(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m conv_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minv-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid4()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m resp1 = \u001b[38;5;28;01mawait\u001b[39;00m client.responses.create(\n\u001b[32m    100\u001b[39m     model=DEPLOYMENT_NAME,\n\u001b[32m    101\u001b[39m     instructions=system_prompt,\n\u001b[32m    102\u001b[39m     \u001b[38;5;28minput\u001b[39m=(\n\u001b[32m    103\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mStep 1 — SOLUTION REASONING:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReason step by step to extract the structured invoice data from the text below. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    105\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse slow thinking. Do NOT output the final answer yet.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- INVOICE TEXT START ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- INVOICE TEXT END ---\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     ),\n\u001b[32m    108\u001b[39m     reasoning={\u001b[33m\"\u001b[39m\u001b[33meffort\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m    109\u001b[39m     text={\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m    110\u001b[39m     max_output_tokens=\u001b[32m4000\u001b[39m,\n\u001b[32m    111\u001b[39m     conversation=conv_id,\n\u001b[32m    112\u001b[39m     store=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# <-- required so the ID can be referenced\u001b[39;00m\n\u001b[32m    113\u001b[39m )\n\u001b[32m    115\u001b[39m resp2 = \u001b[38;5;28;01mawait\u001b[39;00m client.responses.create(\n\u001b[32m    116\u001b[39m     model=DEPLOYMENT_NAME,\n\u001b[32m    117\u001b[39m     previous_response_id=resp1.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m     store=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# <-- keep stored for step 3 to find it\u001b[39;00m\n\u001b[32m    127\u001b[39m )\n\u001b[32m    129\u001b[39m resp3 = \u001b[38;5;28;01mawait\u001b[39;00m client.responses.create(\n\u001b[32m    130\u001b[39m     model=DEPLOYMENT_NAME,\n\u001b[32m    131\u001b[39m     previous_response_id=resp2.id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m     store=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    146\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\Desktop\\LLM-Confidence\\Code\\.venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:2259\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2223\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2224\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2257\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2258\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2260\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2261\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2262\u001b[39m             {\n\u001b[32m   2263\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2264\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2265\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2266\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2267\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2268\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2269\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2270\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2271\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2272\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2273\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2274\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2275\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2276\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2277\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2278\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2279\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2280\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2281\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2282\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2283\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2284\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2285\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2286\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2287\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2288\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2289\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2290\u001b[39m             },\n\u001b[32m   2291\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2292\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2294\u001b[39m         ),\n\u001b[32m   2295\u001b[39m         options=make_request_options(\n\u001b[32m   2296\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2297\u001b[39m         ),\n\u001b[32m   2298\u001b[39m         cast_to=Response,\n\u001b[32m   2299\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2300\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\Desktop\\LLM-Confidence\\Code\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonas\\Desktop\\LLM-Confidence\\Code\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Unknown parameter: ''.\", 'type': 'invalid_request_error', 'param': None, 'code': 'unknown_parameter'}}"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Block 2.1 — Confidence + Self-Consistency (unified)\n",
    "# Requires Block 1 variables: client, DEPLOYMENT_NAME, system_prompt, schema, CONCCURRENT_TASKS\n",
    "# =========================\n",
    "\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import json\n",
    "import re\n",
    "from uuid import uuid4\n",
    "\n",
    "# ----- Verbal classes -----\n",
    "CONFIDENCE_CLASSES = [\n",
    "    \"Almost no chance\",\n",
    "    \"Highly unlikely\",\n",
    "    \"Chances are slight\",\n",
    "    \"Unlikely\",\n",
    "    \"Less than even\",\n",
    "    \"Better than even\",\n",
    "    \"Likely\",\n",
    "    \"Very good chance\",\n",
    "    \"Highly likely\",\n",
    "    \"Almost certain\",\n",
    "]\n",
    "\n",
    "# Numeric → verbal bins (inclusive lower bounds)\n",
    "_SCORE_BOUNDS = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "\n",
    "def score_to_verbal(score: float) -> Optional[str]:\n",
    "    \"\"\"Map [0,1] numeric score to one of 10 verbal classes.\"\"\"\n",
    "    if score is None or not isinstance(score, (int, float)):\n",
    "        return None\n",
    "    s = max(0.0, min(1.0, float(score)))\n",
    "    idx = max(i for i, b in enumerate(_SCORE_BOUNDS) if s >= b)\n",
    "    return CONFIDENCE_CLASSES[idx]\n",
    "\n",
    "# ----- Helpers for reasoning text capture -----\n",
    "def _extract_reasoning_text(resp) -> str:\n",
    "    chunks = []\n",
    "    for item in getattr(resp, \"output\", []) or []:\n",
    "        if getattr(item, \"type\", \"\") == \"reasoning\":\n",
    "            summary = getattr(item, \"summary\", None)\n",
    "            if isinstance(summary, list):\n",
    "                for seg in summary:\n",
    "                    t = getattr(seg, \"text\", None)\n",
    "                    if t:\n",
    "                        chunks.append(t)\n",
    "            for seg in getattr(item, \"content\", []) or []:\n",
    "                t = getattr(seg, \"text\", None)\n",
    "                if t:\n",
    "                    chunks.append(t)\n",
    "    if chunks:\n",
    "        return \"\\n\".join(chunks).strip()\n",
    "    return getattr(resp, \"output_text\", \"\") or \"\"\n",
    "\n",
    "def _coerce_answer_dict(ans: dict) -> dict:\n",
    "    \"\"\"\n",
    "    If the model returned a schema-like wrapper with 'values', unwrap it.\n",
    "    Otherwise return dict as-is.\n",
    "    \"\"\"\n",
    "    if isinstance(ans, dict) and \"properties\" in ans and \"values\" in ans:\n",
    "        v = ans.get(\"values\")\n",
    "        return v if isinstance(v, dict) else ans\n",
    "    return ans if isinstance(ans, dict) else {}\n",
    "\n",
    "def _postprocess_final_payload(payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Expected payload:\n",
    "      {\n",
    "        \"answer\": { ... },                   # keys per your schema (dot-notation allowed)\n",
    "        \"confidence\": { \"Field.Key\": 0..1 }  # per-field numeric confidences\n",
    "      }\n",
    "    Returns:\n",
    "      {\n",
    "        \"answer\": {...},\n",
    "        \"confidence_numeric\": {...},\n",
    "        \"confidence_verbal\": {...},\n",
    "      }\n",
    "    \"\"\"\n",
    "    answer = _coerce_answer_dict(payload.get(\"answer\", {}))\n",
    "    conf_num = payload.get(\"confidence\", {}) or {}\n",
    "    conf_verbal = {k: score_to_verbal(v) for k, v in conf_num.items()}\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"confidence_numeric\": conf_num,\n",
    "        \"confidence_verbal\": conf_verbal,\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# Single-pass, per-field numeric confidence\n",
    "# =========================\n",
    "async def run_three_step_invoice_with_reasoning(invoice_txt_path: str, output_dir: Path):\n",
    "    text = Path(invoice_txt_path).read_text(encoding=\"utf-8\")\n",
    "    conv_id = f\"inv-{uuid4()}\"\n",
    "\n",
    "    resp1 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        instructions=system_prompt,\n",
    "        input=(\n",
    "            \"Step 1 — SOLUTION REASONING:\\n\"\n",
    "            \"Reason step by step to extract the structured invoice data from the text below. \"\n",
    "            \"Use slow thinking. Do NOT output the final answer yet.\\n\\n\"\n",
    "            f\"--- INVOICE TEXT START ---\\n{text}\\n--- INVOICE TEXT END ---\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        text={\"verbosity\": \"low\"},\n",
    "        max_output_tokens=4000,\n",
    "        conversation=conv_id,\n",
    "        store=True,  # <-- required so the ID can be referenced\n",
    "    )\n",
    "\n",
    "    resp2 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        previous_response_id=resp1.id,\n",
    "        input=(\n",
    "            \"Step 2 — CONFIDENCE REASONING:\\n\"\n",
    "            \"Evaluate how likely each extracted field is correct. Do NOT output the final answer yet.\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        text={\"verbosity\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        conversation=conv_id,\n",
    "        store=True,  # <-- keep stored for step 3 to find it\n",
    "    )\n",
    "\n",
    "    resp3 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        previous_response_id=resp2.id,\n",
    "        input=(\n",
    "            \"Step 3 — FINAL OUTPUT:\\n\"\n",
    "            \"Return ONLY a single valid JSON object:\\n\"\n",
    "            \"{\\n\"\n",
    "            f'  \"answer\": <your final extracted invoice data strictly following this schema: {json.dumps(schema)}>,\\n'\n",
    "            '  \"confidence\": { /* per-field numeric [0,1], keys mirror \"answer\"; dot-notation for nested */ }\\n'\n",
    "            \"}\\n\"\n",
    "            \"No markdown. No extra keys. No comments in the JSON.\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        text={\"verbosity\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        conversation=conv_id,\n",
    "        store=False,\n",
    "    )\n",
    "\n",
    "    # Strict JSON parse with light fallback\n",
    "    try:\n",
    "        payload = json.loads(resp3.output_text)\n",
    "    except json.JSONDecodeError:\n",
    "        cleaned = resp3.output_text.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.strip(\"`\")\n",
    "            cleaned = cleaned[cleaned.find(\"{\") : cleaned.rfind(\"}\") + 1]\n",
    "        payload = json.loads(cleaned)\n",
    "\n",
    "    processed = _postprocess_final_payload(payload)\n",
    "\n",
    "    lean_record = {\n",
    "        \"input_path\": str(invoice_txt_path),\n",
    "        \"answer\": processed[\"answer\"],\n",
    "        \"confidence_numeric\": processed[\"confidence_numeric\"],\n",
    "        \"confidence_verbal\": processed[\"confidence_verbal\"],\n",
    "        \"reasoning\": {\n",
    "            \"step1\": _extract_reasoning_text(resp1),\n",
    "            \"step2\": _extract_reasoning_text(resp2),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    out_path = output_dir / (Path(invoice_txt_path).stem + \".json\")\n",
    "    out_path.write_text(json.dumps(lean_record, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return lean_record\n",
    "\n",
    "async def run_many_invoices(invoice_txt_paths):\n",
    "    run_ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = Path(f\"../Output/{run_ts}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sem = asyncio.Semaphore(CONCCURRENT_TASKS)\n",
    "\n",
    "    async def _bounded(p):\n",
    "        async with sem:\n",
    "            return await run_three_step_invoice_with_reasoning(p, output_dir)\n",
    "\n",
    "    tasks = [asyncio.create_task(_bounded(p)) for p in invoice_txt_paths]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    (output_dir / \"_index.json\").write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return {\"output_dir\": str(output_dir), \"results\": results}\n",
    "\n",
    "# =========================\n",
    "# Self-consistency: multi-trace + weighted voting\n",
    "# =========================\n",
    "\n",
    "def _norm_str(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip()).casefold()\n",
    "\n",
    "def _round_num(x: float, ndigits: int = 2) -> float:\n",
    "    try:\n",
    "        return round(float(x), ndigits)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def _value_key(v: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Canonicalize values to stable keys for voting.\n",
    "    - numbers rounded\n",
    "    - strings normalized\n",
    "    - lists/tuples/dicts converted to sortable tuples\n",
    "    \"\"\"\n",
    "    if isinstance(v, (int, float)):\n",
    "        return _round_num(v, 2)\n",
    "    if isinstance(v, str):\n",
    "        return _norm_str(v)\n",
    "    if isinstance(v, list):\n",
    "        return tuple(_value_key(x) for x in v)\n",
    "    if isinstance(v, tuple):\n",
    "        return tuple(_value_key(x) for x in v)\n",
    "    if isinstance(v, dict):\n",
    "        return tuple(sorted((k, _value_key(val)) for k, val in v.items()))\n",
    "    return v\n",
    "\n",
    "def _pick_representative(raw_values: List[Any], winner_key: Any) -> Any:\n",
    "    \"\"\"Pick a representative original value for the winning bucket.\"\"\"\n",
    "    candidates = [v for v in raw_values if _value_key(v) == winner_key]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    cnt = Counter(json.dumps(v, ensure_ascii=False, sort_keys=True) for v in candidates)\n",
    "    rep_json = cnt.most_common(1)[0][0]\n",
    "    return json.loads(rep_json)\n",
    "\n",
    "def _weighted_vote_per_field(results: List[Dict[str, Any]]) -> Tuple[Dict[str, Any], Dict[str, float], Dict[str, str], Dict[str, float], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    results: list of trace dicts with keys:\n",
    "      - answer: dict of field -> value\n",
    "      - confidence_numeric: dict of field -> weight in [0,1]\n",
    "    Returns:\n",
    "      consensus_answer, consensus_conf_num, consensus_conf_verbal, agreement_ratio, support_count\n",
    "    \"\"\"\n",
    "    all_fields = set()\n",
    "    for r in results:\n",
    "        all_fields.update(r.get(\"answer\", {}).keys())\n",
    "\n",
    "    consensus_answer = {}\n",
    "    consensus_conf_num = {}\n",
    "    consensus_conf_verbal = {}\n",
    "    agreement_ratio = {}\n",
    "    support_count = {}\n",
    "\n",
    "    for field in sorted(all_fields):\n",
    "        buckets = defaultdict(float)\n",
    "        raw_vals = []\n",
    "        total_weight = 0.0\n",
    "        votes = 0\n",
    "\n",
    "        for r in results:\n",
    "            ans = r.get(\"answer\", {})\n",
    "            confs = r.get(\"confidence_numeric\", {})\n",
    "            if field in ans:\n",
    "                v = ans[field]\n",
    "                w = float(confs.get(field, 0.0) or 0.0)\n",
    "                k = _value_key(v)\n",
    "                w_clipped = max(0.0, min(1.0, w))\n",
    "                buckets[k] += w_clipped\n",
    "                raw_vals.append(v)\n",
    "                total_weight += w_clipped\n",
    "                votes += 1\n",
    "\n",
    "        if not buckets:\n",
    "            continue\n",
    "\n",
    "        # Winner by max total weight\n",
    "        best_key, best_weight = max(buckets.items(), key=lambda kv: (kv[1],))\n",
    "        if total_weight > 0:\n",
    "            agree = best_weight / total_weight\n",
    "        else:\n",
    "            # Fallback: unweighted majority\n",
    "            count_buckets = defaultdict(int)\n",
    "            for v in raw_vals:\n",
    "                count_buckets[_value_key(v)] += 1\n",
    "            best_key = max(count_buckets.items(), key=lambda kv: (kv[1],))[0]\n",
    "            agree = count_buckets[best_key] / max(1, votes)\n",
    "\n",
    "        rep_value = _pick_representative(raw_vals, best_key)\n",
    "        consensus_answer[field] = rep_value\n",
    "        conf_num = max(0.0, min(1.0, agree))\n",
    "        consensus_conf_num[field] = conf_num\n",
    "        consensus_conf_verbal[field] = score_to_verbal(conf_num)\n",
    "        agreement_ratio[field] = agree\n",
    "        support_count[field] = votes\n",
    "\n",
    "    return consensus_answer, consensus_conf_num, consensus_conf_verbal, agreement_ratio, support_count\n",
    "\n",
    "async def _run_single_trace(invoice_txt_path: str) -> Dict[str, Any]:\n",
    "    text = Path(invoice_txt_path).read_text(encoding=\"utf-8\")\n",
    "    conv_id = f\"inv-{uuid4()}\"\n",
    "\n",
    "    resp1 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        instructions=system_prompt,\n",
    "        input=(\n",
    "            \"Step 1 — SOLUTION REASONING:\\n\"\n",
    "            \"Reason step by step to extract the structured invoice data. \"\n",
    "            \"Do NOT output the final answer yet.\\n\\n\"\n",
    "            f\"--- INVOICE TEXT START ---\\n{text}\\n--- INVOICE TEXT END ---\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        text={\"verbosity\": \"low\"},\n",
    "        max_output_tokens=4000,\n",
    "        conversation=conv_id,\n",
    "        store=True,\n",
    "    )\n",
    "\n",
    "    resp2 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        previous_response_id=resp1.id,\n",
    "        input=\"Step 2 — CONFIDENCE REASONING:\\nAssess per-field likelihoods. Do NOT output the final answer.\",\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        text={\"verbosity\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        conversation=conv_id,\n",
    "        store=True,\n",
    "    )\n",
    "\n",
    "    resp3 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        previous_response_id=resp2.id,\n",
    "        input=(\n",
    "            \"Step 3 — FINAL OUTPUT:\\n\"\n",
    "            \"{\\n\"\n",
    "            f'  \"answer\": <schema: {json.dumps(schema)}>,\\n'\n",
    "            '  \"confidence\": { /* per-field numeric [0,1], keys mirror \"answer\"; dot-notation for nested */ }\\n'\n",
    "            \"}\\n\"\n",
    "            \"No markdown. No extra text.\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        text={\"verbosity\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        conversation=conv_id,\n",
    "        store=False,\n",
    "    )\n",
    "\n",
    "    payload = json.loads(resp3.output_text if resp3.output_text.strip().startswith(\"{\")\n",
    "                         else resp3.output_text.strip()[resp3.output_text.find(\"{\"):resp3.output_text.rfind(\"}\")+1])\n",
    "    processed = _postprocess_final_payload(payload)\n",
    "    return {\n",
    "        \"answer\": processed[\"answer\"],\n",
    "        \"confidence_numeric\": processed[\"confidence_numeric\"],\n",
    "        \"reasoning\": {\n",
    "            \"step1\": _extract_reasoning_text(resp1),\n",
    "            \"step2\": _extract_reasoning_text(resp2),\n",
    "        },\n",
    "        \"raw_output_text\": resp3.output_text,\n",
    "    }\n",
    "\n",
    "async def run_self_consistent_invoice(invoice_txt_path: str, n_paths: int = 5):\n",
    "    \"\"\"\n",
    "    Multi-trace self-consistency for one document.\n",
    "    Weighted voting per field using internal confidences.\n",
    "    \"\"\"\n",
    "    run_ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = Path(f\"../Output/{run_ts}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    sem = asyncio.Semaphore(CONCCURRENT_TASKS)\n",
    "\n",
    "    async def _bounded():\n",
    "        async with sem:\n",
    "            return await _run_single_trace(invoice_txt_path)\n",
    "\n",
    "    traces = await asyncio.gather(*[asyncio.create_task(_bounded()) for _ in range(n_paths)])\n",
    "\n",
    "\n",
    "    # Persist individual traces\n",
    "    traces_dir = output_dir / \"traces\"\n",
    "    traces_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, tr in enumerate(traces, 1):\n",
    "        (traces_dir / f\"trace_{i:02d}.json\").write_text(json.dumps(tr, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Consensus\n",
    "    ans, conf_num, conf_verbal, agree_ratio, support = _weighted_vote_per_field(traces)\n",
    "\n",
    "    final_record = {\n",
    "        \"input_path\": str(invoice_txt_path),\n",
    "        \"consensus\": {\n",
    "            \"answer\": ans,\n",
    "            \"confidence_numeric\": conf_num,\n",
    "            \"confidence_verbal\": conf_verbal,\n",
    "            \"agreement_ratio\": agree_ratio,   # weighted agreement 0..1\n",
    "            \"support_count\": support          # number of traces that produced a value per field\n",
    "        },\n",
    "        \"meta\": {\n",
    "            \"n_paths\": n_paths,\n",
    "            \"output_dir\": str(output_dir),\n",
    "        },\n",
    "        \"traces\": traces,  # per-trace summaries\n",
    "    }\n",
    "\n",
    "    (output_dir / (Path(invoice_txt_path).stem + \"_consensus.json\")).write_text(\n",
    "        json.dumps(final_record, ensure_ascii=False, indent=2), encoding=\"utf-8\"\n",
    "    )\n",
    "    return final_record\n",
    "\n",
    "# ===== Examples =====\n",
    "# Single-pass for many files:\n",
    "paths = [\"../Documents/273366/273277_page1_grid.txt\"]\n",
    "out = await run_many_invoices(paths)\n",
    "\n",
    "print (\"intermediate done\")\n",
    "# Self-consistency for one file:\n",
    "consensus = await run_self_consistent_invoice(\"../Documents/273366/273277_page1_grid.txt\", n_paths=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
