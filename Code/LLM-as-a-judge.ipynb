{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22c8dcc",
   "metadata": {},
   "source": [
    "# Implementation Roadmap\n",
    "## LLM as a judge\n",
    "### Must-have (ship first)\n",
    "- **Strict JSON schema + retry-on-fail**  \n",
    "  Ensures well-formed, consistent outputs.  \n",
    "\n",
    "- **Multi-trace CoT (k=3–5) + self-confidence**  \n",
    "  Generate multiple reasoning traces; each trace provides its own confidence score.  \n",
    "\n",
    "- **Weighted voting by self-confidence**  \n",
    "  Aggregate values using confidence weights instead of plain majority.  \n",
    "\n",
    "- **Evidence grounding (span linking)**  \n",
    "  Judge must cite exact supporting text spans; missing evidence → lower confidence.  \n",
    "\n",
    "- **Domain validators (hard/soft rules)**  \n",
    "  - Dates follow expected formats  \n",
    "  - VAT ID / IBAN checksum validation  \n",
    "  - `Gross ≈ Net + VAT` (within tolerance)  \n",
    "  - Currency whitelist checks  \n",
    "\n",
    "- **Per-field calibration**  \n",
    "  Map raw confidence to true correctness probability (e.g., isotonic or temperature scaling).  \n",
    "\n",
    "- **Acceptance policy**  \n",
    "  Apply per-field thresholds; for documents, require all critical fields to pass.  \n",
    "\n",
    "- **Auditing & provenance**  \n",
    "  Store chosen value, calibrated confidence, cited spans, validator outcomes, and reasoning traces.  \n",
    "\n",
    "---\n",
    "\n",
    "### High-impact add-ons (still no training)\n",
    "- **Dual-phase reasoning**  \n",
    "  1. Solution reasoning → generate answer  \n",
    "  2. Confidence reasoning → judge and verbalize confidence bin (0–9 or 0–1)  \n",
    "\n",
    "- **Difficulty-aware sampling**  \n",
    "  Use vote dispersion or evidence quality to stop early on easy fields or sample more traces for hard ones.  \n",
    "\n",
    "- **Cross-judge diversity (multiple rubrics)**  \n",
    "  Run the same model with different judging rubrics and combine results:  \n",
    "  - *Format-strict judge*: strict regex/format checks (e.g., dates, VAT IDs)  \n",
    "  - *Arithmetic-strict judge*: check totals and sums (e.g., Gross vs. Net + VAT)  \n",
    "  - *Evidence-strict judge*: require supporting spans near relevant anchors (e.g., “VAT”, “Total”)  \n",
    "\n",
    "- **Few-shot bank (prompt-only)**  \n",
    "  Maintain a curated set of vendor/layout examples; update as new reviewed cases are added.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ops & monitoring\n",
    "- **Coverage vs. accuracy dashboards**  \n",
    "  Track Brier, ECE, NLL, AUROC; monitor auto-accept vs. review rates.  \n",
    "\n",
    "- **Periodic recalibration**  \n",
    "  Refresh calibration mappings as new reviewed cases arrive.  \n",
    "\n",
    "- **Fail-safes**  \n",
    "  If JSON invalid or confidence unstable → deterministic re-ask at temperature=0; else route to review.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052830f",
   "metadata": {},
   "source": [
    "<style>\n",
    "open { color: Red }\n",
    "inprogress { color: Yellow }\n",
    "done { color: Green; text-decoration: line-through }\n",
    "</style>\n",
    "\n",
    "# First steps\n",
    "To start things off, we need to build a simple LLM-as-a-Judge (LaaJ) setup<br>\n",
    "For this we need to do prepare the following:\n",
    "- <done>Access to the Azure hosted GPT5-mini</done>\n",
    "- <inprogress>Access to the data located on the NAS</inprogress>\n",
    "\n",
    "Then we need to force the LLM to create multiple reasoning paths for one input.<br>\n",
    "- <done>Actively encourage the LLM to perform \"slow thinking\" and prepare a json output schema the data needs to fit in.</done><br>\n",
    "- <done>Implement a retry option (with amount of retries as a parameter) to catch LLM errors.</done><br>\n",
    "- <open>Peform validity checks for produced outputs (Valid VAT layout, isNumber for amounts, ...). If these checks fail --> confidence decrease / set to 0.</open><br>\n",
    "- <open>Query the LLM to rate its own output using numeric and written bins (10 bins, [0.0; 0.1], [0.1; 0.2], ...)</open><br>\n",
    "- <open>Implement weighted majority voting based on confidence (Borda voting with exponent)</open><br>\n",
    "\n",
    "<open>Visualize the results of perceived confidence and actual correctness in a bar chart (should be x=y for perfect prediction)</open><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653a805",
   "metadata": {},
   "source": [
    "# LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0648315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from openai import AsyncOpenAI, RateLimitError\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv(override = True)\n",
    "# ---------------------------------HELPER-VARS--------------------------------\n",
    "DOCUMENT_STORAGE = Path(os.getenv(\"DOCUMENT_STORAGE\"))\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# ---------------------------------LLM-CONFIG---------------------------------\n",
    "API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# ---------------------------------LLM-SETUP---------------------------------\n",
    "client = AsyncOpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=ENDPOINT\n",
    ")\n",
    "\n",
    "CONCCURRENT_TASKS = 15\n",
    "REQUEST_DELAY = 1 # seconds\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 5\n",
    "RETRY_BACKOFF = 5  # seconds\n",
    "\n",
    "# Whitelist of retryable exceptions\n",
    "RETRYABLE_EXCEPTIONS = (\n",
    "    RateLimitError,\n",
    "    httpx.ConnectTimeout,\n",
    "    httpx.ReadTimeout,\n",
    "    httpx.HTTPStatusError,\n",
    "    httpx.RemoteProtocolError,\n",
    "    httpx.NetworkError,\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------LOGGING---------------------------------\n",
    "logging_path = Path(\"../Logs\")\n",
    "if not logging_path.exists():\n",
    "    logging_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=f\"{logging_path}/{DEPLOYMENT_NAME}_{TIMESTAMP}.log\",\n",
    "    filemode=\"a\",                     \n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    force=True\n",
    ")\n",
    "\n",
    "for noisy in [\"httpx\", \"openai\", \"azure\", \"urllib3\"]:\n",
    "    logging.getLogger(noisy).setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "# Load system prompt\n",
    "with open(\"../Prompts/system_prompt.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    system_prompt = f.read()\n",
    "\n",
    "# Load the schema from file\n",
    "with open(\"../Prompts/schema.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "# Turn it into pretty JSON for prompt injection\n",
    "formatted_schema = json.dumps(schema, indent=2)\n",
    "\n",
    "system_prompt = system_prompt.replace(\"formatted_schema_representation\",formatted_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acf4a9",
   "metadata": {},
   "source": [
    "# Test Reasoning path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9426837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Verbal classes (kept) ---\n",
    "CONFIDENCE_CLASSES = [\n",
    "    \"Almost no chance\",\n",
    "    \"Highly unlikely\",\n",
    "    \"Chances are slight\",\n",
    "    \"Unlikely\",\n",
    "    \"Less than even\",\n",
    "    \"Better than even\",\n",
    "    \"Likely\",\n",
    "    \"Very good chance\",\n",
    "    \"Highly likely\",\n",
    "    \"Almost certain\"\n",
    "]\n",
    "\n",
    "# --- Numeric→verbal mapping via bins (inclusive lower bounds) ---\n",
    "# [0.00,0.10) -> Almost no chance, [0.10,0.20) -> Highly unlikely, ..., [0.90,1.00] -> Almost certain\n",
    "_SCORE_BOUNDS = [0.00, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]\n",
    "def score_to_verbal(score: float) -> str:\n",
    "    if score is None or not isinstance(score, (int, float)):\n",
    "        return None\n",
    "    s = max(0.0, min(1.0, float(score)))\n",
    "    idx = max(i for i, b in enumerate(_SCORE_BOUNDS) if s >= b)  # choose the last bound ≤ s\n",
    "    return CONFIDENCE_CLASSES[idx]\n",
    "\n",
    "def _extract_reasoning_text(resp) -> str:\n",
    "    chunks = []\n",
    "    for item in getattr(resp, \"output\", []) or []:\n",
    "        if getattr(item, \"type\", \"\") == \"reasoning\":\n",
    "            summary = getattr(item, \"summary\", None)\n",
    "            if isinstance(summary, list):\n",
    "                for seg in summary:\n",
    "                    t = getattr(seg, \"text\", None)\n",
    "                    if t:\n",
    "                        chunks.append(t)\n",
    "            for seg in getattr(item, \"content\", []) or []:\n",
    "                t = getattr(seg, \"text\", None)\n",
    "                if t:\n",
    "                    chunks.append(t)\n",
    "    if chunks:\n",
    "        return \"\\n\".join(chunks).strip()\n",
    "    return getattr(resp, \"output_text\", \"\") or \"\"\n",
    "\n",
    "def _coerce_answer_dict(ans: dict) -> dict:\n",
    "    if isinstance(ans, dict) and \"properties\" in ans and \"values\" in ans:\n",
    "        v = ans.get(\"values\")\n",
    "        return v if isinstance(v, dict) else ans\n",
    "    return ans if isinstance(ans, dict) else {}\n",
    "\n",
    "def _postprocess_final_payload(payload: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Expecting payload shape:\n",
    "    {\n",
    "      \"answer\": {...},                       # dict conforming to your schema\n",
    "      \"confidence\": {\"field.path\": float}    # 0..1 per field that appears in 'answer'\n",
    "    }\n",
    "    Returns lean record with numeric + verbal per field.\n",
    "    \"\"\"\n",
    "    answer = _coerce_answer_dict(payload.get(\"answer\", {}))\n",
    "    conf_num = payload.get(\"confidence\", {}) or {}\n",
    "\n",
    "    # Build verbal per field for fields present in numeric dict\n",
    "    conf_verbal = {k: score_to_verbal(v) for k, v in conf_num.items()}\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"confidence_numeric\": conf_num,\n",
    "        \"confidence_verbal\": conf_verbal,\n",
    "    }\n",
    "\n",
    "async def run_three_step_invoice_with_reasoning(invoice_txt_path: str, output_dir: Path):\n",
    "    text = Path(invoice_txt_path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "    # Step 1 — SOLUTION REASONING\n",
    "    resp1 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        instructions=system_prompt,\n",
    "        input=(\n",
    "            \"Step 1 — SOLUTION REASONING:\\n\"\n",
    "            \"Reason step by step to extract the structured invoice data from the text below. \"\n",
    "            \"Use slow thinking (explore alternatives, verify fields). \"\n",
    "            \"Do NOT output the final answer yet.\\n\\n\"\n",
    "            f\"--- INVOICE TEXT START ---\\n{text}\\n--- INVOICE TEXT END ---\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        max_output_tokens=4000,\n",
    "        text={\"verbosity\": \"low\"},\n",
    "    )\n",
    "\n",
    "    # Step 2 — CONFIDENCE REASONING\n",
    "    resp2 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        previous_response_id=resp1.id,\n",
    "        input=(\n",
    "            \"Step 2 — CONFIDENCE REASONING:\\n\"\n",
    "            \"Evaluate, step by step, how likely each extracted field is correct, based on internal consistency \"\n",
    "            \"and evidence in the text. Do NOT output the final answer yet.\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        text={\"verbosity\": \"low\"},\n",
    "    )\n",
    "\n",
    "    # Step 3 — FINAL JSON WITH PER-FIELD NUMERIC CONFIDENCE\n",
    "    resp3 = await client.responses.create(\n",
    "        model=DEPLOYMENT_NAME,\n",
    "        previous_response_id=resp2.id,\n",
    "        input=(\n",
    "            \"Step 3 — FINAL OUTPUT:\\n\"\n",
    "            \"Return ONLY a single valid JSON object with this structure and nothing else:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"answer\": <your final extracted invoice data strictly following this schema: '\n",
    "            f\"{json.dumps(schema)}\"\n",
    "            \">,\\n\"\n",
    "            '  \"confidence\": {\\n'\n",
    "            \"    // For every field you output in 'answer', provide a numeric confidence in [0,1].\\n\"\n",
    "            '    // Use the exact same field keys as in \"answer\". For nested fields, use dot-notation keys.\\n'\n",
    "            '    // Example: \"Net.Amount\": 0.82\\n'\n",
    "            \"  }\\n\"\n",
    "            \"}\\n\"\n",
    "            \"Do not include comments in the actual JSON. Do not include any extra keys. No markdown.\"\n",
    "        ),\n",
    "        reasoning={\"effort\": \"low\"},\n",
    "        max_output_tokens=2000,\n",
    "        text={\"verbosity\": \"low\"},\n",
    "    )\n",
    "\n",
    "    # Parse strict JSON\n",
    "    try:\n",
    "        payload = json.loads(resp3.output_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: try to strip code fences or accidental text\n",
    "        cleaned = resp3.output_text.strip()\n",
    "        if cleaned.startswith(\"```\"):\n",
    "            cleaned = cleaned.strip(\"`\")\n",
    "            cleaned = cleaned[cleaned.find(\"{\") : cleaned.rfind(\"}\") + 1]\n",
    "        payload = json.loads(cleaned)\n",
    "\n",
    "    processed = _postprocess_final_payload(payload)\n",
    "\n",
    "    lean_record = {\n",
    "        \"input_path\": str(invoice_txt_path),\n",
    "        \"answer\": processed[\"answer\"],\n",
    "        \"confidence_numeric\": processed[\"confidence_numeric\"],\n",
    "        \"confidence_verbal\": processed[\"confidence_verbal\"],\n",
    "        \"reasoning\": {\n",
    "            \"step1\": _extract_reasoning_text(resp1),\n",
    "            \"step2\": _extract_reasoning_text(resp2),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    out_path = output_dir / (Path(invoice_txt_path).stem + \".json\")\n",
    "    out_path.write_text(json.dumps(lean_record, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return lean_record\n",
    "\n",
    "async def run_many_invoices(invoice_txt_paths):\n",
    "    run_ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    output_dir = Path(f\"../Output/{run_ts}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sem = asyncio.Semaphore(CONCCURRENT_TASKS)\n",
    "\n",
    "    async def _bounded(path):\n",
    "        async with sem:\n",
    "            return await run_three_step_invoice_with_reasoning(path, output_dir)\n",
    "\n",
    "    tasks = [asyncio.create_task(_bounded(p)) for p in invoice_txt_paths]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    (output_dir / \"_index.json\").write_text(json.dumps(results, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    return {\"output_dir\": str(output_dir), \"results\": results}\n",
    "\n",
    "# Example:\n",
    "paths = [\"../Documents/273366/273277_page1_grid.txt\"]\n",
    "out = await run_many_invoices(paths)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
