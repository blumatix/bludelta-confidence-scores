{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c259130d",
   "metadata": {},
   "source": [
    "# 2 Ways of predicting confidence\n",
    "## 1. LLM as a judge\n",
    "One or more (Majority Voting) LLMs judge the output generated and reach a final verdict on a [0;1] scale (0 = unusable, 1 = perfect)\n",
    "\n",
    "## 2. Logprobs\n",
    "The probability of an output token is inspected<br>\n",
    "If it is high enough (threshhold per sample), the judgment is most likely good\n",
    "\n",
    "\n",
    "# Notes\n",
    "Derzeit immer 0.85 als default Wert von Typhoon<br>\n",
    "Implementierung via Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5249646",
   "metadata": {},
   "source": [
    "# Open Questions\n",
    "- Which fields are inspected\n",
    "    - If multiple fields, global score or score per field? --> per field\n",
    "    - If aggregated field (VAT Group), global or per sub-item? --> per field\n",
    "\n",
    "\n",
    "- Which LLMs to use\n",
    "    - OpenAI --> Which versions (4.1-mini, 5.x?) --> 5 mini \n",
    "    - Azure Document AI? --> eigener Mistral account (Derzeit noch nicht verfügbar)\n",
    "    - Azure AI Document intelligence --> Research what this can do\n",
    "\n",
    "- Do tests with RAG or with base setup? --> Without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010a3a1",
   "metadata": {},
   "source": [
    "# Related Works\n",
    "## LLM as a judge\n",
    "### Self consistency\n",
    "<b>Link:</b>https://arxiv.org/pdf/2203.11171<br>\n",
    "<b>Description:</b> Origin paper for self consistency.<br>\n",
    "Using CoT-reasoning and multiple thinking paths, a single LLM can try diverse answer paths.<br>\n",
    "Most of the time, the correct answer is the one that most paths agree upon --> Majority voting with self-consistency\n",
    "\n",
    "### Reasoning for confidence\n",
    "<b>Link:</b> https://arxiv.org/pdf/2505.14489v1<br>\n",
    "<b>Description:</b> This paper proposes that <i>slow thinking</i> helps the LLM to better judge its own confidence for an answer.<br>\n",
    "The backtracking abilites and uncertainty helps develop multiple paths --> good for self-consistency as well. <br>\n",
    "For CoT prompting they use:\n",
    "1. Solution reasoning: Reasoning to generate the answer\n",
    "2. Confidence reasoning: Reasoning to evaluate own confidence\n",
    "3. Confidence verbalization: Put confidence into 1 of 10 bins [0;1]\n",
    "\n",
    "For evaluation, Brier Score is used, which helps to combine Excpected Calibration Error (ECE) and the Aread under the ROC curve (AUROC) score<br>\n",
    "Slow thinking apparently works better in larger models!\n",
    "\n",
    "## Logprobs\n",
    "### DeepConf\n",
    "<b>Link:</b> https://arxiv.org/pdf/2508.15260 <br>\n",
    "<b>Description:</b> DeepConf tries to optimize self-consistency (majority voting on multiple reasoning paths), as this <i>parallel thinking</i> is very token intensive.<br>\n",
    "In addition, performance degrades as all reasoning paths are weighted equally<br>\n",
    "DeepConf can:\n",
    " - Evaluate reasoning traces \n",
    " - Abort low quality traces\n",
    " - Set a task adaptive threshold for \"low quality\" (Offline warmup)\n",
    " - Grade the difficuluty of a task by comparing vote shares in majority voting (Adaptive Sampling)\n",
    " <br>\n",
    "![Overview of DeepConf with Sample Code](../Images/Research/DeepConfOverview.png)\n",
    "\n",
    "### llm_confidence Package\n",
    "<b>Link:</b>https://medium.com/%40vatvenger/confidence-unlocked-a-method-to-measure-certainty-in-llm-outputs-1d921a4ca43c<br>\n",
    "<b>Description:</b> This medium post explains the working of log probs and introduces a python package to work with them. <br>\n",
    "Log probs are:\n",
    "- logarithmic\n",
    "- Work best in key-value pairs\n",
    "- can be summed up (all tokens) per pair to get a confidence score for the pair<br>\n",
    "\n",
    "In this test, differences have been uncovered when extracting fields individually, or grouped together in a json dict.<br>\n",
    "\n",
    "\n",
    "### Temperature scaling\n",
    "<b>Link:</b>https://arxiv.org/pdf/2409.19817<br>\n",
    "<b>Description:</b> After RLHF(Reinfocment Learning with Human Feedback) the calibration of LLMs (accuracy to confidence ratio) is off.<br>\n",
    "With ATS (Adaptive Temperature Scaling) this paper tries to restore the balance, in a task adaptive manner.<br>\n",
    "The token-level probabilites should be restored\n",
    "\n",
    "### Self-certainty\n",
    "<b>Link:</b>https://arxiv.org/pdf/2502.18581<br>\n",
    "<b>Description:</b> A successor to self-consistency, self-certainty measures the deviation of the probability distribution from a uniform distribution.<br>\n",
    "By assigning more votes to voters with a higher confidence score, a weighted majority voting system is implemented\n",
    "\n",
    "\n",
    "## Additional papers\n",
    "HYCEDIS (2022) — Combines multimodal confidence predictors + anomaly detection for invoices and receipts.\n",
    "\n",
    "PatchFinder (2024) — Uses patch-wise max-softmax uncertainty for scanned receipts, robust against noisy OCR.\n",
    "\n",
    "KIEval (2025) — Defines evaluation metrics linking confidence thresholds to automation rate in KIE pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f600ddd",
   "metadata": {},
   "source": [
    "# Additional information\n",
    "For this experiment we will use the \"grid\" layout .txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c8dcc",
   "metadata": {},
   "source": [
    "# Implementation Roadmap\n",
    "## LLM as a judge\n",
    "### Must-have (ship first)\n",
    "- **Strict JSON schema + retry-on-fail**  \n",
    "  Ensures well-formed, consistent outputs.  \n",
    "\n",
    "- **Multi-trace CoT (k=3–5) + self-confidence**  \n",
    "  Generate multiple reasoning traces; each trace provides its own confidence score.  \n",
    "\n",
    "- **Weighted voting by self-confidence**  \n",
    "  Aggregate values using confidence weights instead of plain majority.  \n",
    "\n",
    "- **Evidence grounding (span linking)**  \n",
    "  Judge must cite exact supporting text spans; missing evidence → lower confidence.  \n",
    "\n",
    "- **Domain validators (hard/soft rules)**  \n",
    "  - Dates follow expected formats  \n",
    "  - VAT ID / IBAN checksum validation  \n",
    "  - `Gross ≈ Net + VAT` (within tolerance)  \n",
    "  - Currency whitelist checks  \n",
    "\n",
    "- **Per-field calibration**  \n",
    "  Map raw confidence to true correctness probability (e.g., isotonic or temperature scaling).  \n",
    "\n",
    "- **Acceptance policy**  \n",
    "  Apply per-field thresholds; for documents, require all critical fields to pass.  \n",
    "\n",
    "- **Auditing & provenance**  \n",
    "  Store chosen value, calibrated confidence, cited spans, validator outcomes, and reasoning traces.  \n",
    "\n",
    "---\n",
    "\n",
    "### High-impact add-ons (still no training)\n",
    "- **Dual-phase reasoning**  \n",
    "  1. Solution reasoning → generate answer  \n",
    "  2. Confidence reasoning → judge and verbalize confidence bin (0–9 or 0–1)  \n",
    "\n",
    "- **Difficulty-aware sampling**  \n",
    "  Use vote dispersion or evidence quality to stop early on easy fields or sample more traces for hard ones.  \n",
    "\n",
    "- **Cross-judge diversity (multiple rubrics)**  \n",
    "  Run the same model with different judging rubrics and combine results:  \n",
    "  - *Format-strict judge*: strict regex/format checks (e.g., dates, VAT IDs)  \n",
    "  - *Arithmetic-strict judge*: check totals and sums (e.g., Gross vs. Net + VAT)  \n",
    "  - *Evidence-strict judge*: require supporting spans near relevant anchors (e.g., “VAT”, “Total”)  \n",
    "\n",
    "- **Few-shot bank (prompt-only)**  \n",
    "  Maintain a curated set of vendor/layout examples; update as new reviewed cases are added.  \n",
    "\n",
    "---\n",
    "\n",
    "### Ops & monitoring\n",
    "- **Coverage vs. accuracy dashboards**  \n",
    "  Track Brier, ECE, NLL, AUROC; monitor auto-accept vs. review rates.  \n",
    "\n",
    "- **Periodic recalibration**  \n",
    "  Refresh calibration mappings as new reviewed cases arrive.  \n",
    "\n",
    "- **Fail-safes**  \n",
    "  If JSON invalid or confidence unstable → deterministic re-ask at temperature=0; else route to review.  \n",
    "\n",
    "\n",
    "## Logprobs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
